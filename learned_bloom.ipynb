{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Bloom Filter\n",
    "\n",
    "A learned bloom filter uses a `learned oracle` as a pre-filtering stage in front of a standard bloom filter, called the back-up bloom filter. The oracle function $f(x)$ represents $Pr[x \\in S]$. We set a threshold $\\tau$ such that if $f(x) \\geq \\tau$, we return `True` for the query. Otherwise, if $f(x) < \\tau$, we send the query to the back-up bloom filter. The back-up bloom filters job is to eliminate the possibility of getting false negatives. \n",
    "\n",
    "Given a set of elements $S$ from universe $U$, we can train the oracle using any ML model on the binary classification task with negative log likelihood loss function. The training dataset contains `(query, label)` pairs, with both positive and negative samples. \n",
    "\n",
    "Positive samples are all the elements from $S$ and negative samples are all the elements $U\\setminus S$.\n",
    "\n",
    "The back-up bloom filter is only meant to hold the subset of positive keys $\\set{x \\in S : f(x)<\\tau}$ which are mis-classified by the oracle (i.e. False Negatives), instead of all keys in $S$, which can lead to significant space savings compared to a standard bloom filter on entire $S$.\n",
    "\n",
    "We keep a held-out set of negative samples $Q$ with which we tune the threshold $\\tau$, and the backup-bloom filter parameters, to acheive a desired false positive rate $\\delta$. (More on this later...)\n",
    "\n",
    "The goal is to create a 2-stage bloom filter that consumes less space than a standard bloom filter while achieving the same (or better) false positive rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import sympy as sp\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoUniversalHashFamily:\n",
    "    def __init__(self, m, max_key):\n",
    "        self.m = m  # Size of the hash table\n",
    "        self.p = sp.nextprime(max_key)  # generate a large prime number, greater than any key\n",
    "        self.a = random.randint(1, self.p-1)  # Choose a randomly\n",
    "        self.b = random.randint(0, self.p-1)  # Choose b randomly\n",
    "\n",
    "    def hash(self, k):\n",
    "        return ((self.a * k + self.b) % self.p) % self.m\n",
    "    \n",
    "    def __call__(self, k):\n",
    "        return self.hash(k)\n",
    "\n",
    "\n",
    "class BloomStandard:\n",
    "    def __init__(self, S, m, k=None, max_key=10000000):\n",
    "        self.m = m\n",
    "        if k is None:\n",
    "            self.k = max(1,round((m/len(S)) * math.log(2)))  # optimal number of hash functions for a given m and n \n",
    "            print(f\"Optimal number of hash functions: {self.k}\")      \n",
    "        else:\n",
    "            self.k = k\n",
    "        # draw k random hash functions from universal hash family\n",
    "        self.h = [TwoUniversalHashFamily(m, max_key) for _ in range(self.k)]\n",
    "        self.B = [0] * self.m\n",
    "\n",
    "        # construct bit array\n",
    "        for key in S:\n",
    "            self.insert(key)\n",
    "        print(f\"Bloom filter constructed! Size: {self.m}, Number of hash functions: {self.k}\")\n",
    "\n",
    "    # insert new integer key into the bloom filter \n",
    "    def insert(self, key):\n",
    "        for i in range(self.k):\n",
    "            self.B[self.h[i](key)] = 1\n",
    "\n",
    "    # poerform membership query for the given key\n",
    "    def query(self, key):\n",
    "        q = [0]*self.k\n",
    "        for i in range(self.k):\n",
    "            q[i] = self.B[self.h[i](key)]\n",
    "        if 0 in q:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple dataset. Suppose the universe is $U = \\set{0,1,2... 100000-1}$ and $S = \\set{1000,1001,...,2000-1}$, i.e. the positive keys are all the integers in the range[1000,1999]. Then the target oracle function needs to be: \n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } 1000 \\leq x < 2000 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We will now try to learn this target function with a neural network with 1 hidden layer trained on binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 1000\n",
    "usize = 10000\n",
    "\n",
    "# prepare training dataset\n",
    "positive_samples_train = [(x,1) for x in range(1000,2000)]\n",
    "all_negative_samples = [(x,0) for x in (list(range(0,1000)) + list(range(2000,usize)))]\n",
    "# draw n_test negative samples for testing\n",
    "negative_samples_test = random.sample(all_negative_samples, n_test)\n",
    "# prepare training negative samples by removing n_test samples from all_negative_samples\n",
    "negative_samples_train = [x for x in all_negative_samples if x not in negative_samples_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytroch dataset class for training\n",
    "class BloomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, positive_samples, negative_samples):\n",
    "        if positive_samples is None:\n",
    "            positive_samples = []\n",
    "        self.samples = positive_samples + negative_samples\n",
    "        random.shuffle(self.samples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        # convert to tensor\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# define neural network with one hidden layer\n",
    "class Oracle(torch.nn.Module):\n",
    "    def __init__(self, hidden_dims=10, dropout_rate=0.2):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.hidden_layer = torch.nn.Linear(1, hidden_dims)\n",
    "        self.output_layer = torch.nn.Linear(hidden_dims, 2)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.ReLU()(self.hidden_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(train_dataloader, model, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        num_total = 0\n",
    "        num_correct = 0\n",
    "        avg_loss = 0\n",
    "        for batch in pbar:\n",
    "            x, y = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = F.cross_entropy(outputs, y.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute moving average loss\n",
    "            avg_loss = 0.9 * avg_loss + 0.1 * loss.item()\n",
    "\n",
    "            # compute accuracy for batch precictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            accuracy = num_correct / num_total\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.5f}, Train Accuracy: {accuracy: .5f}\")\n",
    "\n",
    "\n",
    "def evaluate(test_dataloader, model):\n",
    "    num_total = 0\n",
    "    num_correct = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "        accuracy = num_correct / num_total\n",
    "    FP_rate = 1 - accuracy    \n",
    "    print(f\"Test FP rate: {FP_rate:.5f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "batch_size = 128\n",
    "hidden_dims = 100\n",
    "num_epochs = 10\n",
    "\n",
    "# define model and optimizer\n",
    "model = Oracle(hidden_dims)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# prepare training dataloader\n",
    "train_dataset = BloomDataset(positive_samples_train, negative_samples_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = BloomDataset(None, negative_samples_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 72.82753, Train Accuracy:  0.68389: 100%|██████████| 71/71 [00:00<00:00, 232.82it/s] \n",
      "Epoch 2, Train Loss: 58.81951, Train Accuracy:  0.84067: 100%|██████████| 71/71 [00:00<00:00, 219.39it/s]\n",
      "Epoch 3, Train Loss: 50.75907, Train Accuracy:  0.85967: 100%|██████████| 71/71 [00:00<00:00, 210.30it/s]\n",
      "Epoch 4, Train Loss: 50.63747, Train Accuracy:  0.85622: 100%|██████████| 71/71 [00:00<00:00, 189.85it/s]\n",
      "Epoch 5, Train Loss: 57.24235, Train Accuracy:  0.86367: 100%|██████████| 71/71 [00:00<00:00, 200.09it/s]\n",
      "Epoch 6, Train Loss: 55.08238, Train Accuracy:  0.85822: 100%|██████████| 71/71 [00:00<00:00, 211.43it/s]\n",
      "Epoch 7, Train Loss: 58.81644, Train Accuracy:  0.85878: 100%|██████████| 71/71 [00:00<00:00, 218.67it/s]\n",
      "Epoch 8, Train Loss: 50.78608, Train Accuracy:  0.85989: 100%|██████████| 71/71 [00:00<00:00, 244.91it/s]\n",
      "Epoch 9, Train Loss: 52.70173, Train Accuracy:  0.86244: 100%|██████████| 71/71 [00:00<00:00, 223.91it/s]\n",
      "Epoch 10, Train Loss: 47.13574, Train Accuracy:  0.86144: 100%|██████████| 71/71 [00:00<00:00, 260.57it/s]\n",
      "Epoch 11, Train Loss: 43.36042, Train Accuracy:  0.85833: 100%|██████████| 71/71 [00:00<00:00, 224.07it/s]\n",
      "Epoch 12, Train Loss: 46.82302, Train Accuracy:  0.85956: 100%|██████████| 71/71 [00:00<00:00, 237.76it/s]\n",
      "Epoch 13, Train Loss: 41.28216, Train Accuracy:  0.86111: 100%|██████████| 71/71 [00:00<00:00, 246.51it/s]\n",
      "Epoch 14, Train Loss: 44.33830, Train Accuracy:  0.85933: 100%|██████████| 71/71 [00:00<00:00, 232.82it/s]\n",
      "Epoch 15, Train Loss: 41.14950, Train Accuracy:  0.86011: 100%|██████████| 71/71 [00:00<00:00, 217.39it/s]\n",
      "Epoch 16, Train Loss: 39.02490, Train Accuracy:  0.86244: 100%|██████████| 71/71 [00:00<00:00, 217.60it/s]\n",
      "Epoch 17, Train Loss: 38.52107, Train Accuracy:  0.86122: 100%|██████████| 71/71 [00:00<00:00, 240.20it/s]\n",
      "Epoch 18, Train Loss: 35.33568, Train Accuracy:  0.85822: 100%|██████████| 71/71 [00:00<00:00, 237.81it/s]\n",
      "Epoch 19, Train Loss: 36.91616, Train Accuracy:  0.86278: 100%|██████████| 71/71 [00:00<00:00, 218.69it/s]\n",
      "Epoch 20, Train Loss: 34.21297, Train Accuracy:  0.86067: 100%|██████████| 71/71 [00:00<00:00, 222.83it/s]\n",
      "Epoch 21, Train Loss: 31.24429, Train Accuracy:  0.85689: 100%|██████████| 71/71 [00:00<00:00, 193.38it/s]\n",
      "Epoch 22, Train Loss: 27.37551, Train Accuracy:  0.86000: 100%|██████████| 71/71 [00:00<00:00, 216.59it/s]\n",
      "Epoch 23, Train Loss: 29.87727, Train Accuracy:  0.86100: 100%|██████████| 71/71 [00:00<00:00, 241.44it/s]\n",
      "Epoch 24, Train Loss: 27.36890, Train Accuracy:  0.86044: 100%|██████████| 71/71 [00:00<00:00, 237.29it/s]\n",
      "Epoch 25, Train Loss: 23.29969, Train Accuracy:  0.85989: 100%|██████████| 71/71 [00:00<00:00, 225.10it/s]\n",
      "Epoch 26, Train Loss: 25.04754, Train Accuracy:  0.86256: 100%|██████████| 71/71 [00:00<00:00, 215.87it/s]\n",
      "Epoch 27, Train Loss: 22.44870, Train Accuracy:  0.85878: 100%|██████████| 71/71 [00:00<00:00, 229.47it/s]\n",
      "Epoch 28, Train Loss: 21.78398, Train Accuracy:  0.85956: 100%|██████████| 71/71 [00:00<00:00, 251.51it/s]\n",
      "Epoch 29, Train Loss: 20.62670, Train Accuracy:  0.86233: 100%|██████████| 71/71 [00:00<00:00, 230.27it/s]\n",
      "Epoch 30, Train Loss: 18.00358, Train Accuracy:  0.85789: 100%|██████████| 71/71 [00:00<00:00, 243.89it/s]\n",
      "Epoch 31, Train Loss: 14.51739, Train Accuracy:  0.86067: 100%|██████████| 71/71 [00:00<00:00, 240.04it/s]\n",
      "Epoch 32, Train Loss: 17.74239, Train Accuracy:  0.86089: 100%|██████████| 71/71 [00:00<00:00, 244.97it/s]\n",
      "Epoch 33, Train Loss: 12.76351, Train Accuracy:  0.86011: 100%|██████████| 71/71 [00:00<00:00, 239.64it/s]\n",
      "Epoch 34, Train Loss: 11.40407, Train Accuracy:  0.86033: 100%|██████████| 71/71 [00:00<00:00, 225.67it/s]\n",
      "Epoch 35, Train Loss: 11.23507, Train Accuracy:  0.85878: 100%|██████████| 71/71 [00:00<00:00, 246.07it/s]\n",
      "Epoch 36, Train Loss: 8.94467, Train Accuracy:  0.85989: 100%|██████████| 71/71 [00:00<00:00, 236.92it/s] \n",
      "Epoch 37, Train Loss: 8.40180, Train Accuracy:  0.85911: 100%|██████████| 71/71 [00:00<00:00, 242.99it/s]\n",
      "Epoch 38, Train Loss: 7.12997, Train Accuracy:  0.85922: 100%|██████████| 71/71 [00:00<00:00, 238.88it/s]\n",
      "Epoch 39, Train Loss: 5.69941, Train Accuracy:  0.85978: 100%|██████████| 71/71 [00:00<00:00, 232.46it/s]\n",
      "Epoch 40, Train Loss: 5.21951, Train Accuracy:  0.86200: 100%|██████████| 71/71 [00:00<00:00, 225.12it/s]\n",
      "Epoch 41, Train Loss: 4.84512, Train Accuracy:  0.86033: 100%|██████████| 71/71 [00:00<00:00, 235.07it/s]\n",
      "Epoch 42, Train Loss: 4.13010, Train Accuracy:  0.85844: 100%|██████████| 71/71 [00:00<00:00, 234.99it/s]\n",
      "Epoch 43, Train Loss: 3.29524, Train Accuracy:  0.86322: 100%|██████████| 71/71 [00:00<00:00, 238.56it/s]\n",
      "Epoch 44, Train Loss: 2.75315, Train Accuracy:  0.85878: 100%|██████████| 71/71 [00:00<00:00, 222.72it/s]\n",
      "Epoch 45, Train Loss: 2.26037, Train Accuracy:  0.85967: 100%|██████████| 71/71 [00:00<00:00, 234.94it/s]\n",
      "Epoch 46, Train Loss: 1.73906, Train Accuracy:  0.86078: 100%|██████████| 71/71 [00:00<00:00, 236.44it/s]\n",
      "Epoch 47, Train Loss: 1.47147, Train Accuracy:  0.86256: 100%|██████████| 71/71 [00:00<00:00, 229.86it/s]\n",
      "Epoch 48, Train Loss: 1.24998, Train Accuracy:  0.86211: 100%|██████████| 71/71 [00:00<00:00, 231.10it/s]\n",
      "Epoch 49, Train Loss: 1.01848, Train Accuracy:  0.85933: 100%|██████████| 71/71 [00:00<00:00, 233.28it/s]\n",
      "Epoch 50, Train Loss: 0.70106, Train Accuracy:  0.86122: 100%|██████████| 71/71 [00:00<00:00, 235.87it/s]\n",
      "Epoch 51, Train Loss: 0.63329, Train Accuracy:  0.86389: 100%|██████████| 71/71 [00:00<00:00, 246.92it/s]\n",
      "Epoch 52, Train Loss: 0.66136, Train Accuracy:  0.86367: 100%|██████████| 71/71 [00:00<00:00, 214.84it/s]\n",
      "Epoch 53, Train Loss: 0.72323, Train Accuracy:  0.86378: 100%|██████████| 71/71 [00:00<00:00, 224.70it/s]\n",
      "Epoch 54, Train Loss: 0.63793, Train Accuracy:  0.86756: 100%|██████████| 71/71 [00:00<00:00, 223.53it/s]\n",
      "Epoch 55, Train Loss: 0.38239, Train Accuracy:  0.86967: 100%|██████████| 71/71 [00:00<00:00, 228.38it/s]\n",
      "Epoch 56, Train Loss: 0.32506, Train Accuracy:  0.86878: 100%|██████████| 71/71 [00:00<00:00, 202.85it/s]\n",
      "Epoch 57, Train Loss: 0.47821, Train Accuracy:  0.86822: 100%|██████████| 71/71 [00:00<00:00, 221.63it/s]\n",
      "Epoch 58, Train Loss: 2.27215, Train Accuracy:  0.85911: 100%|██████████| 71/71 [00:00<00:00, 227.22it/s]\n",
      "Epoch 59, Train Loss: 0.77443, Train Accuracy:  0.86789: 100%|██████████| 71/71 [00:00<00:00, 235.48it/s]\n",
      "Epoch 60, Train Loss: 0.25975, Train Accuracy:  0.87289: 100%|██████████| 71/71 [00:00<00:00, 218.37it/s]\n",
      "Epoch 61, Train Loss: 0.22796, Train Accuracy:  0.87778: 100%|██████████| 71/71 [00:00<00:00, 229.66it/s]\n",
      "Epoch 62, Train Loss: 0.20856, Train Accuracy:  0.87778: 100%|██████████| 71/71 [00:00<00:00, 214.78it/s]\n",
      "Epoch 63, Train Loss: 0.23971, Train Accuracy:  0.87700: 100%|██████████| 71/71 [00:00<00:00, 191.24it/s]\n",
      "Epoch 64, Train Loss: 0.21064, Train Accuracy:  0.87867: 100%|██████████| 71/71 [00:00<00:00, 197.62it/s]\n",
      "Epoch 65, Train Loss: 0.22655, Train Accuracy:  0.87756: 100%|██████████| 71/71 [00:00<00:00, 221.66it/s]\n",
      "Epoch 66, Train Loss: 0.21651, Train Accuracy:  0.87778: 100%|██████████| 71/71 [00:00<00:00, 235.32it/s]\n",
      "Epoch 67, Train Loss: 0.21664, Train Accuracy:  0.88044: 100%|██████████| 71/71 [00:00<00:00, 217.10it/s]\n",
      "Epoch 68, Train Loss: 0.34745, Train Accuracy:  0.87678: 100%|██████████| 71/71 [00:00<00:00, 233.45it/s]\n",
      "Epoch 69, Train Loss: 3.22850, Train Accuracy:  0.86167: 100%|██████████| 71/71 [00:00<00:00, 235.13it/s]\n",
      "Epoch 70, Train Loss: 0.70854, Train Accuracy:  0.86456: 100%|██████████| 71/71 [00:00<00:00, 226.01it/s]\n",
      "Epoch 71, Train Loss: 1.79796, Train Accuracy:  0.85800: 100%|██████████| 71/71 [00:00<00:00, 246.97it/s]\n",
      "Epoch 72, Train Loss: 0.90490, Train Accuracy:  0.86556: 100%|██████████| 71/71 [00:00<00:00, 207.81it/s]\n",
      "Epoch 73, Train Loss: 0.96255, Train Accuracy:  0.86333: 100%|██████████| 71/71 [00:00<00:00, 224.73it/s]\n",
      "Epoch 74, Train Loss: 0.26551, Train Accuracy:  0.87578: 100%|██████████| 71/71 [00:00<00:00, 243.16it/s]\n",
      "Epoch 75, Train Loss: 0.20695, Train Accuracy:  0.87622: 100%|██████████| 71/71 [00:00<00:00, 248.45it/s]\n",
      "Epoch 76, Train Loss: 0.21618, Train Accuracy:  0.87622: 100%|██████████| 71/71 [00:00<00:00, 225.42it/s]\n",
      "Epoch 77, Train Loss: 0.21323, Train Accuracy:  0.87600: 100%|██████████| 71/71 [00:00<00:00, 254.05it/s]\n",
      "Epoch 78, Train Loss: 0.20538, Train Accuracy:  0.87611: 100%|██████████| 71/71 [00:00<00:00, 231.63it/s]\n",
      "Epoch 79, Train Loss: 0.20699, Train Accuracy:  0.87644: 100%|██████████| 71/71 [00:00<00:00, 225.65it/s]\n",
      "Epoch 80, Train Loss: 0.19030, Train Accuracy:  0.87533: 100%|██████████| 71/71 [00:00<00:00, 238.67it/s]\n",
      "Epoch 81, Train Loss: 0.18231, Train Accuracy:  0.87744: 100%|██████████| 71/71 [00:00<00:00, 239.29it/s]\n",
      "Epoch 82, Train Loss: 0.22031, Train Accuracy:  0.87656: 100%|██████████| 71/71 [00:00<00:00, 231.20it/s]\n",
      "Epoch 83, Train Loss: 0.20171, Train Accuracy:  0.87700: 100%|██████████| 71/71 [00:00<00:00, 227.37it/s]\n",
      "Epoch 84, Train Loss: 0.19233, Train Accuracy:  0.87478: 100%|██████████| 71/71 [00:00<00:00, 235.62it/s]\n",
      "Epoch 85, Train Loss: 0.19927, Train Accuracy:  0.87322: 100%|██████████| 71/71 [00:00<00:00, 226.76it/s]\n",
      "Epoch 86, Train Loss: 0.21679, Train Accuracy:  0.87589: 100%|██████████| 71/71 [00:00<00:00, 228.06it/s]\n",
      "Epoch 87, Train Loss: 0.20563, Train Accuracy:  0.87800: 100%|██████████| 71/71 [00:00<00:00, 242.40it/s]\n",
      "Epoch 88, Train Loss: 0.20730, Train Accuracy:  0.87522: 100%|██████████| 71/71 [00:00<00:00, 254.23it/s]\n",
      "Epoch 89, Train Loss: 0.20712, Train Accuracy:  0.87778: 100%|██████████| 71/71 [00:00<00:00, 222.78it/s]\n",
      "Epoch 90, Train Loss: 0.21086, Train Accuracy:  0.87578: 100%|██████████| 71/71 [00:00<00:00, 229.56it/s]\n",
      "Epoch 91, Train Loss: 0.21697, Train Accuracy:  0.87478: 100%|██████████| 71/71 [00:00<00:00, 237.63it/s]\n",
      "Epoch 92, Train Loss: 0.20723, Train Accuracy:  0.87411: 100%|██████████| 71/71 [00:00<00:00, 241.15it/s]\n",
      "Epoch 93, Train Loss: 0.20571, Train Accuracy:  0.87578: 100%|██████████| 71/71 [00:00<00:00, 243.02it/s]\n",
      "Epoch 94, Train Loss: 0.20730, Train Accuracy:  0.87400: 100%|██████████| 71/71 [00:00<00:00, 213.81it/s]\n",
      "Epoch 95, Train Loss: 0.18800, Train Accuracy:  0.87722: 100%|██████████| 71/71 [00:00<00:00, 247.58it/s]\n",
      "Epoch 96, Train Loss: 0.19916, Train Accuracy:  0.88011: 100%|██████████| 71/71 [00:00<00:00, 232.30it/s]\n",
      "Epoch 97, Train Loss: 0.19745, Train Accuracy:  0.88156: 100%|██████████| 71/71 [00:00<00:00, 223.39it/s]\n",
      "Epoch 98, Train Loss: 0.19162, Train Accuracy:  0.88233: 100%|██████████| 71/71 [00:00<00:00, 251.79it/s]\n",
      "Epoch 99, Train Loss: 2.97880, Train Accuracy:  0.86556: 100%|██████████| 71/71 [00:00<00:00, 242.91it/s]\n",
      "Epoch 100, Train Loss: 1.44155, Train Accuracy:  0.86056: 100%|██████████| 71/71 [00:00<00:00, 224.01it/s]\n",
      "Epoch 101, Train Loss: 2.09984, Train Accuracy:  0.86122: 100%|██████████| 71/71 [00:00<00:00, 240.27it/s]\n",
      "Epoch 102, Train Loss: 0.37352, Train Accuracy:  0.87311: 100%|██████████| 71/71 [00:00<00:00, 241.78it/s]\n",
      "Epoch 103, Train Loss: 0.19123, Train Accuracy:  0.88378: 100%|██████████| 71/71 [00:00<00:00, 224.39it/s]\n",
      "Epoch 104, Train Loss: 0.19825, Train Accuracy:  0.88433: 100%|██████████| 71/71 [00:00<00:00, 245.47it/s]\n",
      "Epoch 105, Train Loss: 0.19298, Train Accuracy:  0.88544: 100%|██████████| 71/71 [00:00<00:00, 245.38it/s]\n",
      "Epoch 106, Train Loss: 0.18453, Train Accuracy:  0.88656: 100%|██████████| 71/71 [00:00<00:00, 235.57it/s]\n",
      "Epoch 107, Train Loss: 0.18723, Train Accuracy:  0.88722: 100%|██████████| 71/71 [00:00<00:00, 237.98it/s]\n",
      "Epoch 108, Train Loss: 0.18807, Train Accuracy:  0.88589: 100%|██████████| 71/71 [00:00<00:00, 249.58it/s]\n",
      "Epoch 109, Train Loss: 0.20293, Train Accuracy:  0.88622: 100%|██████████| 71/71 [00:00<00:00, 247.64it/s]\n",
      "Epoch 110, Train Loss: 0.20704, Train Accuracy:  0.88578: 100%|██████████| 71/71 [00:00<00:00, 219.88it/s]\n",
      "Epoch 111, Train Loss: 0.18102, Train Accuracy:  0.88611: 100%|██████████| 71/71 [00:00<00:00, 252.27it/s]\n",
      "Epoch 112, Train Loss: 0.18169, Train Accuracy:  0.88744: 100%|██████████| 71/71 [00:00<00:00, 238.11it/s]\n",
      "Epoch 113, Train Loss: 0.21452, Train Accuracy:  0.89411: 100%|██████████| 71/71 [00:00<00:00, 226.20it/s]\n",
      "Epoch 114, Train Loss: 0.17386, Train Accuracy:  0.88789: 100%|██████████| 71/71 [00:00<00:00, 243.88it/s]\n",
      "Epoch 115, Train Loss: 0.17154, Train Accuracy:  0.88867: 100%|██████████| 71/71 [00:00<00:00, 245.18it/s]\n",
      "Epoch 116, Train Loss: 0.17532, Train Accuracy:  0.88811: 100%|██████████| 71/71 [00:00<00:00, 226.68it/s]\n",
      "Epoch 117, Train Loss: 0.18678, Train Accuracy:  0.88989: 100%|██████████| 71/71 [00:00<00:00, 235.27it/s]\n",
      "Epoch 118, Train Loss: 0.18220, Train Accuracy:  0.88978: 100%|██████████| 71/71 [00:00<00:00, 246.51it/s]\n",
      "Epoch 119, Train Loss: 0.18892, Train Accuracy:  0.88844: 100%|██████████| 71/71 [00:00<00:00, 221.33it/s]\n",
      "Epoch 120, Train Loss: 0.17424, Train Accuracy:  0.88933: 100%|██████████| 71/71 [00:00<00:00, 251.41it/s]\n",
      "Epoch 121, Train Loss: 0.19004, Train Accuracy:  0.88956: 100%|██████████| 71/71 [00:00<00:00, 217.95it/s]\n",
      "Epoch 122, Train Loss: 0.18098, Train Accuracy:  0.88911: 100%|██████████| 71/71 [00:00<00:00, 224.23it/s]\n",
      "Epoch 123, Train Loss: 0.17500, Train Accuracy:  0.89111: 100%|██████████| 71/71 [00:00<00:00, 242.15it/s]\n",
      "Epoch 124, Train Loss: 0.18443, Train Accuracy:  0.88822: 100%|██████████| 71/71 [00:00<00:00, 210.08it/s]\n",
      "Epoch 125, Train Loss: 0.19730, Train Accuracy:  0.89011: 100%|██████████| 71/71 [00:00<00:00, 243.46it/s]\n",
      "Epoch 126, Train Loss: 0.18103, Train Accuracy:  0.88856: 100%|██████████| 71/71 [00:00<00:00, 228.27it/s]\n",
      "Epoch 127, Train Loss: 0.19805, Train Accuracy:  0.89156: 100%|██████████| 71/71 [00:00<00:00, 226.95it/s]\n",
      "Epoch 128, Train Loss: 3.26681, Train Accuracy:  0.86356: 100%|██████████| 71/71 [00:00<00:00, 224.90it/s]\n",
      "Epoch 129, Train Loss: 2.69323, Train Accuracy:  0.85978: 100%|██████████| 71/71 [00:00<00:00, 221.32it/s]\n",
      "Epoch 130, Train Loss: 0.29959, Train Accuracy:  0.87378: 100%|██████████| 71/71 [00:00<00:00, 225.57it/s]\n",
      "Epoch 131, Train Loss: 0.18124, Train Accuracy:  0.89089: 100%|██████████| 71/71 [00:00<00:00, 244.89it/s]\n",
      "Epoch 132, Train Loss: 0.16528, Train Accuracy:  0.88889: 100%|██████████| 71/71 [00:00<00:00, 251.54it/s]\n",
      "Epoch 133, Train Loss: 0.17592, Train Accuracy:  0.89011: 100%|██████████| 71/71 [00:00<00:00, 245.40it/s]\n",
      "Epoch 134, Train Loss: 0.17301, Train Accuracy:  0.89267: 100%|██████████| 71/71 [00:00<00:00, 231.78it/s]\n",
      "Epoch 135, Train Loss: 0.16450, Train Accuracy:  0.89067: 100%|██████████| 71/71 [00:00<00:00, 243.16it/s]\n",
      "Epoch 136, Train Loss: 0.17996, Train Accuracy:  0.89333: 100%|██████████| 71/71 [00:00<00:00, 239.97it/s]\n",
      "Epoch 137, Train Loss: 0.18054, Train Accuracy:  0.88922: 100%|██████████| 71/71 [00:00<00:00, 236.14it/s]\n",
      "Epoch 138, Train Loss: 0.19020, Train Accuracy:  0.89278: 100%|██████████| 71/71 [00:00<00:00, 246.37it/s]\n",
      "Epoch 139, Train Loss: 0.17149, Train Accuracy:  0.89422: 100%|██████████| 71/71 [00:00<00:00, 245.67it/s]\n",
      "Epoch 140, Train Loss: 0.17871, Train Accuracy:  0.88933: 100%|██████████| 71/71 [00:00<00:00, 236.20it/s]\n",
      "Epoch 141, Train Loss: 0.18265, Train Accuracy:  0.89233: 100%|██████████| 71/71 [00:00<00:00, 256.46it/s]\n",
      "Epoch 142, Train Loss: 0.17942, Train Accuracy:  0.89200: 100%|██████████| 71/71 [00:00<00:00, 237.86it/s]\n",
      "Epoch 143, Train Loss: 0.20352, Train Accuracy:  0.89256: 100%|██████████| 71/71 [00:00<00:00, 234.79it/s]\n",
      "Epoch 144, Train Loss: 0.16522, Train Accuracy:  0.89467: 100%|██████████| 71/71 [00:00<00:00, 234.10it/s]\n",
      "Epoch 145, Train Loss: 0.16537, Train Accuracy:  0.89033: 100%|██████████| 71/71 [00:00<00:00, 245.33it/s]\n",
      "Epoch 146, Train Loss: 0.16522, Train Accuracy:  0.89167: 100%|██████████| 71/71 [00:00<00:00, 224.47it/s]\n",
      "Epoch 147, Train Loss: 0.15739, Train Accuracy:  0.89178: 100%|██████████| 71/71 [00:00<00:00, 248.74it/s]\n",
      "Epoch 148, Train Loss: 0.18043, Train Accuracy:  0.89022: 100%|██████████| 71/71 [00:00<00:00, 253.39it/s]\n",
      "Epoch 149, Train Loss: 0.18484, Train Accuracy:  0.89422: 100%|██████████| 71/71 [00:00<00:00, 246.48it/s]\n",
      "Epoch 150, Train Loss: 1.88470, Train Accuracy:  0.87856: 100%|██████████| 71/71 [00:00<00:00, 229.09it/s]\n",
      "Epoch 151, Train Loss: 0.27003, Train Accuracy:  0.87222: 100%|██████████| 71/71 [00:00<00:00, 241.76it/s]\n",
      "Epoch 152, Train Loss: 0.17309, Train Accuracy:  0.89200: 100%|██████████| 71/71 [00:00<00:00, 258.94it/s]\n",
      "Epoch 153, Train Loss: 0.17160, Train Accuracy:  0.89256: 100%|██████████| 71/71 [00:00<00:00, 229.93it/s]\n",
      "Epoch 154, Train Loss: 0.16661, Train Accuracy:  0.89556: 100%|██████████| 71/71 [00:00<00:00, 258.65it/s]\n",
      "Epoch 155, Train Loss: 0.17926, Train Accuracy:  0.89389: 100%|██████████| 71/71 [00:00<00:00, 274.38it/s]\n",
      "Epoch 156, Train Loss: 0.17787, Train Accuracy:  0.89433: 100%|██████████| 71/71 [00:00<00:00, 228.35it/s]\n",
      "Epoch 157, Train Loss: 0.16332, Train Accuracy:  0.89622: 100%|██████████| 71/71 [00:00<00:00, 236.88it/s]\n",
      "Epoch 158, Train Loss: 0.16215, Train Accuracy:  0.89411: 100%|██████████| 71/71 [00:00<00:00, 229.17it/s]\n",
      "Epoch 159, Train Loss: 0.17679, Train Accuracy:  0.89589: 100%|██████████| 71/71 [00:00<00:00, 219.59it/s]\n",
      "Epoch 160, Train Loss: 0.16116, Train Accuracy:  0.89689: 100%|██████████| 71/71 [00:00<00:00, 240.82it/s]\n",
      "Epoch 161, Train Loss: 0.15482, Train Accuracy:  0.89422: 100%|██████████| 71/71 [00:00<00:00, 243.00it/s]\n",
      "Epoch 162, Train Loss: 0.19733, Train Accuracy:  0.89744: 100%|██████████| 71/71 [00:00<00:00, 203.15it/s]\n",
      "Epoch 163, Train Loss: 0.17736, Train Accuracy:  0.89867: 100%|██████████| 71/71 [00:00<00:00, 223.21it/s]\n",
      "Epoch 164, Train Loss: 0.17155, Train Accuracy:  0.89800: 100%|██████████| 71/71 [00:00<00:00, 249.50it/s]\n",
      "Epoch 165, Train Loss: 0.16299, Train Accuracy:  0.89811: 100%|██████████| 71/71 [00:00<00:00, 236.04it/s]\n",
      "Epoch 166, Train Loss: 0.17110, Train Accuracy:  0.89800: 100%|██████████| 71/71 [00:00<00:00, 242.51it/s]\n",
      "Epoch 167, Train Loss: 0.17754, Train Accuracy:  0.89922: 100%|██████████| 71/71 [00:00<00:00, 232.18it/s]\n",
      "Epoch 168, Train Loss: 0.16671, Train Accuracy:  0.90111: 100%|██████████| 71/71 [00:00<00:00, 254.17it/s]\n",
      "Epoch 169, Train Loss: 0.16504, Train Accuracy:  0.89911: 100%|██████████| 71/71 [00:00<00:00, 219.50it/s]\n",
      "Epoch 170, Train Loss: 0.16252, Train Accuracy:  0.89400: 100%|██████████| 71/71 [00:00<00:00, 217.26it/s]\n",
      "Epoch 171, Train Loss: 0.17908, Train Accuracy:  0.90300: 100%|██████████| 71/71 [00:00<00:00, 226.89it/s]\n",
      "Epoch 172, Train Loss: 0.16559, Train Accuracy:  0.90044: 100%|██████████| 71/71 [00:00<00:00, 215.33it/s]\n",
      "Epoch 173, Train Loss: 0.16659, Train Accuracy:  0.89711: 100%|██████████| 71/71 [00:00<00:00, 223.15it/s]\n",
      "Epoch 174, Train Loss: 4.58228, Train Accuracy:  0.88656: 100%|██████████| 71/71 [00:00<00:00, 234.00it/s]\n",
      "Epoch 175, Train Loss: 1.31486, Train Accuracy:  0.86922: 100%|██████████| 71/71 [00:00<00:00, 213.86it/s]\n",
      "Epoch 176, Train Loss: 0.28324, Train Accuracy:  0.88133: 100%|██████████| 71/71 [00:00<00:00, 228.42it/s]\n",
      "Epoch 177, Train Loss: 0.15942, Train Accuracy:  0.90400: 100%|██████████| 71/71 [00:00<00:00, 230.88it/s]\n",
      "Epoch 178, Train Loss: 0.16627, Train Accuracy:  0.90533: 100%|██████████| 71/71 [00:00<00:00, 227.51it/s]\n",
      "Epoch 179, Train Loss: 0.16921, Train Accuracy:  0.90522: 100%|██████████| 71/71 [00:00<00:00, 227.20it/s]\n",
      "Epoch 180, Train Loss: 0.17512, Train Accuracy:  0.90478: 100%|██████████| 71/71 [00:00<00:00, 210.28it/s]\n",
      "Epoch 181, Train Loss: 0.16808, Train Accuracy:  0.90300: 100%|██████████| 71/71 [00:00<00:00, 239.88it/s]\n",
      "Epoch 182, Train Loss: 0.16756, Train Accuracy:  0.90111: 100%|██████████| 71/71 [00:00<00:00, 217.17it/s]\n",
      "Epoch 183, Train Loss: 0.17107, Train Accuracy:  0.90544: 100%|██████████| 71/71 [00:00<00:00, 228.38it/s]\n",
      "Epoch 184, Train Loss: 0.16725, Train Accuracy:  0.90267: 100%|██████████| 71/71 [00:00<00:00, 259.10it/s]\n",
      "Epoch 185, Train Loss: 0.16611, Train Accuracy:  0.90956: 100%|██████████| 71/71 [00:00<00:00, 229.43it/s]\n",
      "Epoch 186, Train Loss: 0.15887, Train Accuracy:  0.90556: 100%|██████████| 71/71 [00:00<00:00, 250.37it/s]\n",
      "Epoch 187, Train Loss: 0.16669, Train Accuracy:  0.90611: 100%|██████████| 71/71 [00:00<00:00, 227.55it/s]\n",
      "Epoch 188, Train Loss: 0.16497, Train Accuracy:  0.90533: 100%|██████████| 71/71 [00:00<00:00, 221.56it/s]\n",
      "Epoch 189, Train Loss: 0.15865, Train Accuracy:  0.90533: 100%|██████████| 71/71 [00:00<00:00, 234.17it/s]\n",
      "Epoch 190, Train Loss: 0.15755, Train Accuracy:  0.90489: 100%|██████████| 71/71 [00:00<00:00, 233.17it/s]\n",
      "Epoch 191, Train Loss: 0.15696, Train Accuracy:  0.90878: 100%|██████████| 71/71 [00:00<00:00, 252.47it/s]\n",
      "Epoch 192, Train Loss: 0.13596, Train Accuracy:  0.91078: 100%|██████████| 71/71 [00:00<00:00, 211.46it/s]\n",
      "Epoch 193, Train Loss: 0.17725, Train Accuracy:  0.90933: 100%|██████████| 71/71 [00:00<00:00, 219.64it/s]\n",
      "Epoch 194, Train Loss: 0.14699, Train Accuracy:  0.90667: 100%|██████████| 71/71 [00:00<00:00, 212.22it/s]\n",
      "Epoch 195, Train Loss: 0.14540, Train Accuracy:  0.90289: 100%|██████████| 71/71 [00:00<00:00, 194.27it/s]\n",
      "Epoch 196, Train Loss: 0.15585, Train Accuracy:  0.91144: 100%|██████████| 71/71 [00:00<00:00, 227.65it/s]\n",
      "Epoch 197, Train Loss: 0.17219, Train Accuracy:  0.90700: 100%|██████████| 71/71 [00:00<00:00, 218.71it/s]\n",
      "Epoch 198, Train Loss: 0.15449, Train Accuracy:  0.91089: 100%|██████████| 71/71 [00:00<00:00, 241.18it/s]\n",
      "Epoch 199, Train Loss: 0.16184, Train Accuracy:  0.91056: 100%|██████████| 71/71 [00:00<00:00, 229.53it/s]\n",
      "Epoch 200, Train Loss: 0.15794, Train Accuracy:  0.90800: 100%|██████████| 71/71 [00:00<00:00, 217.70it/s]\n",
      "Epoch 201, Train Loss: 0.15535, Train Accuracy:  0.91367: 100%|██████████| 71/71 [00:00<00:00, 236.18it/s]\n",
      "Epoch 202, Train Loss: 0.23988, Train Accuracy:  0.90822: 100%|██████████| 71/71 [00:00<00:00, 221.11it/s]\n",
      "Epoch 203, Train Loss: 0.15384, Train Accuracy:  0.89467: 100%|██████████| 71/71 [00:00<00:00, 234.75it/s]\n",
      "Epoch 204, Train Loss: 0.14748, Train Accuracy:  0.91378: 100%|██████████| 71/71 [00:00<00:00, 231.16it/s]\n",
      "Epoch 205, Train Loss: 0.28675, Train Accuracy:  0.90333: 100%|██████████| 71/71 [00:00<00:00, 216.99it/s]\n",
      "Epoch 206, Train Loss: 0.15705, Train Accuracy:  0.91400: 100%|██████████| 71/71 [00:00<00:00, 220.68it/s]\n",
      "Epoch 207, Train Loss: 0.16568, Train Accuracy:  0.91589: 100%|██████████| 71/71 [00:00<00:00, 231.70it/s]\n",
      "Epoch 208, Train Loss: 0.15306, Train Accuracy:  0.91278: 100%|██████████| 71/71 [00:00<00:00, 231.03it/s]\n",
      "Epoch 209, Train Loss: 0.13677, Train Accuracy:  0.92089: 100%|██████████| 71/71 [00:00<00:00, 212.50it/s]\n",
      "Epoch 210, Train Loss: 0.13623, Train Accuracy:  0.91611: 100%|██████████| 71/71 [00:00<00:00, 247.31it/s]\n",
      "Epoch 211, Train Loss: 0.17468, Train Accuracy:  0.91467: 100%|██████████| 71/71 [00:00<00:00, 235.28it/s]\n",
      "Epoch 212, Train Loss: 0.18053, Train Accuracy:  0.90178: 100%|██████████| 71/71 [00:00<00:00, 218.82it/s]\n",
      "Epoch 213, Train Loss: 0.14656, Train Accuracy:  0.91433: 100%|██████████| 71/71 [00:00<00:00, 241.87it/s]\n",
      "Epoch 214, Train Loss: 0.14556, Train Accuracy:  0.91944: 100%|██████████| 71/71 [00:00<00:00, 238.97it/s]\n",
      "Epoch 215, Train Loss: 0.14224, Train Accuracy:  0.91800: 100%|██████████| 71/71 [00:00<00:00, 218.03it/s]\n",
      "Epoch 216, Train Loss: 0.27844, Train Accuracy:  0.90622: 100%|██████████| 71/71 [00:00<00:00, 217.17it/s]\n",
      "Epoch 217, Train Loss: 2.45175, Train Accuracy:  0.86767: 100%|██████████| 71/71 [00:00<00:00, 243.56it/s]\n",
      "Epoch 218, Train Loss: 1.47059, Train Accuracy:  0.86011: 100%|██████████| 71/71 [00:00<00:00, 213.23it/s]\n",
      "Epoch 219, Train Loss: 0.16537, Train Accuracy:  0.88911: 100%|██████████| 71/71 [00:00<00:00, 224.69it/s]\n",
      "Epoch 220, Train Loss: 0.12610, Train Accuracy:  0.92633: 100%|██████████| 71/71 [00:00<00:00, 210.72it/s]\n",
      "Epoch 221, Train Loss: 0.13253, Train Accuracy:  0.92689: 100%|██████████| 71/71 [00:00<00:00, 213.58it/s]\n",
      "Epoch 222, Train Loss: 0.14222, Train Accuracy:  0.92956: 100%|██████████| 71/71 [00:00<00:00, 221.85it/s]\n",
      "Epoch 223, Train Loss: 0.13159, Train Accuracy:  0.92800: 100%|██████████| 71/71 [00:00<00:00, 224.71it/s]\n",
      "Epoch 224, Train Loss: 0.14247, Train Accuracy:  0.93144: 100%|██████████| 71/71 [00:00<00:00, 242.98it/s]\n",
      "Epoch 225, Train Loss: 0.14339, Train Accuracy:  0.92789: 100%|██████████| 71/71 [00:00<00:00, 235.75it/s]\n",
      "Epoch 226, Train Loss: 0.13248, Train Accuracy:  0.92911: 100%|██████████| 71/71 [00:00<00:00, 221.54it/s]\n",
      "Epoch 227, Train Loss: 0.14503, Train Accuracy:  0.92844: 100%|██████████| 71/71 [00:00<00:00, 213.92it/s]\n",
      "Epoch 228, Train Loss: 0.12844, Train Accuracy:  0.93111: 100%|██████████| 71/71 [00:00<00:00, 221.08it/s]\n",
      "Epoch 229, Train Loss: 0.13133, Train Accuracy:  0.92811: 100%|██████████| 71/71 [00:00<00:00, 229.65it/s]\n",
      "Epoch 230, Train Loss: 0.13181, Train Accuracy:  0.93067: 100%|██████████| 71/71 [00:00<00:00, 227.57it/s]\n",
      "Epoch 231, Train Loss: 0.13112, Train Accuracy:  0.93122: 100%|██████████| 71/71 [00:00<00:00, 221.38it/s]\n",
      "Epoch 232, Train Loss: 0.15015, Train Accuracy:  0.92589: 100%|██████████| 71/71 [00:00<00:00, 220.80it/s]\n",
      "Epoch 233, Train Loss: 0.13135, Train Accuracy:  0.92956: 100%|██████████| 71/71 [00:00<00:00, 236.07it/s]\n",
      "Epoch 234, Train Loss: 0.13331, Train Accuracy:  0.93289: 100%|██████████| 71/71 [00:00<00:00, 220.24it/s]\n",
      "Epoch 235, Train Loss: 0.12748, Train Accuracy:  0.93222: 100%|██████████| 71/71 [00:00<00:00, 231.95it/s]\n",
      "Epoch 236, Train Loss: 0.13475, Train Accuracy:  0.92533: 100%|██████████| 71/71 [00:00<00:00, 218.65it/s]\n",
      "Epoch 237, Train Loss: 0.13262, Train Accuracy:  0.93378: 100%|██████████| 71/71 [00:00<00:00, 228.21it/s]\n",
      "Epoch 238, Train Loss: 0.14236, Train Accuracy:  0.92633: 100%|██████████| 71/71 [00:00<00:00, 232.05it/s]\n",
      "Epoch 239, Train Loss: 0.13066, Train Accuracy:  0.93278: 100%|██████████| 71/71 [00:00<00:00, 204.08it/s]\n",
      "Epoch 240, Train Loss: 0.14207, Train Accuracy:  0.93222: 100%|██████████| 71/71 [00:00<00:00, 226.32it/s]\n",
      "Epoch 241, Train Loss: 0.13002, Train Accuracy:  0.92622: 100%|██████████| 71/71 [00:00<00:00, 227.91it/s]\n",
      "Epoch 242, Train Loss: 0.13099, Train Accuracy:  0.93544: 100%|██████████| 71/71 [00:00<00:00, 232.73it/s]\n",
      "Epoch 243, Train Loss: 0.13096, Train Accuracy:  0.93344: 100%|██████████| 71/71 [00:00<00:00, 217.68it/s]\n",
      "Epoch 244, Train Loss: 0.14485, Train Accuracy:  0.93133: 100%|██████████| 71/71 [00:00<00:00, 227.88it/s]\n",
      "Epoch 245, Train Loss: 0.14538, Train Accuracy:  0.91811: 100%|██████████| 71/71 [00:00<00:00, 181.51it/s]\n",
      "Epoch 246, Train Loss: 0.13084, Train Accuracy:  0.93478: 100%|██████████| 71/71 [00:00<00:00, 241.94it/s]\n",
      "Epoch 247, Train Loss: 0.13761, Train Accuracy:  0.93211: 100%|██████████| 71/71 [00:00<00:00, 208.35it/s]\n",
      "Epoch 248, Train Loss: 0.13601, Train Accuracy:  0.93422: 100%|██████████| 71/71 [00:00<00:00, 237.12it/s]\n",
      "Epoch 249, Train Loss: 0.12968, Train Accuracy:  0.93300: 100%|██████████| 71/71 [00:00<00:00, 226.79it/s]\n",
      "Epoch 250, Train Loss: 0.13746, Train Accuracy:  0.93589: 100%|██████████| 71/71 [00:00<00:00, 229.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train(train_dataloader, model, optimizer, num_epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 8004, Probs: [0.9997996687889099, 0.00020037156355101615]\n",
      "x: 6884, Probs: [0.9990776777267456, 0.0009223427623510361]\n",
      "x: 9577, Probs: [0.9999765157699585, 2.3426457119057886e-05]\n",
      "x: 7464, Probs: [0.9995817542076111, 0.00041828438406810164]\n",
      "x: 2796, Probs: [0.803819477558136, 0.1961805373430252]\n",
      "x: 8899, Probs: [0.9999408721923828, 5.9064794186269864e-05]\n",
      "x: 5912, Probs: [0.9965351819992065, 0.003464829409494996]\n",
      "x: 4508, Probs: [0.9769375324249268, 0.02306247502565384]\n",
      "x: 8655, Probs: [0.9999176263809204, 8.242293552029878e-05]\n",
      "x: 6482, Probs: [0.9984046816825867, 0.0015953158726915717]\n",
      "x: 819, Probs: [0.7839083671569824, 0.21609164774417877]\n",
      "x: 4466, Probs: [0.9756060242652893, 0.024394024163484573]\n",
      "x: 3802, Probs: [0.941737949848175, 0.05826206132769585]\n",
      "x: 4072, Probs: [0.9589625000953674, 0.041037436574697495]\n",
      "x: 5654, Probs: [0.995080828666687, 0.004919129423797131]\n",
      "x: 6999, Probs: [0.9992111921310425, 0.0007887380779720843]\n",
      "x: 6150, Probs: [0.997492790222168, 0.002507185796275735]\n",
      "x: 9015, Probs: [0.99994957447052, 5.043497003498487e-05]\n",
      "x: 9860, Probs: [0.9999840259552002, 1.592495209479239e-05]\n",
      "x: 6874, Probs: [0.9990649819374084, 0.0009350276086479425]\n",
      "x: 5027, Probs: [0.9885043501853943, 0.011495658196508884]\n",
      "x: 2904, Probs: [0.8260287046432495, 0.1739712804555893]\n",
      "x: 4446, Probs: [0.9749496579170227, 0.025050325319170952]\n",
      "x: 7179, Probs: [0.9993828535079956, 0.0006171565619297326]\n",
      "x: 7383, Probs: [0.99953293800354, 0.0004671200877055526]\n",
      "x: 4686, Probs: [0.9818165302276611, 0.018183467909693718]\n",
      "x: 8848, Probs: [0.9999366998672485, 6.332061457214877e-05]\n",
      "x: 6499, Probs: [0.9984413981437683, 0.0015586079098284245]\n",
      "x: 2646, Probs: [0.7695352435112, 0.23046483099460602]\n",
      "x: 9910, Probs: [0.9999850988388062, 1.487272311351262e-05]\n",
      "x: 4090, Probs: [0.9599173069000244, 0.0400826632976532]\n",
      "x: 8703, Probs: [0.9999227523803711, 7.720306166447699e-05]\n",
      "x: 7777, Probs: [0.9997269511222839, 0.00027302108355797827]\n",
      "x: 208, Probs: [0.9979667663574219, 0.0020332373678684235]\n",
      "x: 7776, Probs: [0.9997267127037048, 0.0002732877619564533]\n",
      "x: 5157, Probs: [0.9903546571731567, 0.009645305573940277]\n",
      "x: 6107, Probs: [0.9973422884941101, 0.0026577545795589685]\n",
      "x: 4500, Probs: [0.9766886830329895, 0.023311305791139603]\n",
      "x: 6628, Probs: [0.9986922144889832, 0.0013078507035970688]\n",
      "x: 9123, Probs: [0.999956488609314, 4.352043833932839e-05]\n",
      "x: 7338, Probs: [0.9995032548904419, 0.0004967466229572892]\n",
      "x: 9526, Probs: [0.9999748468399048, 2.512060927983839e-05]\n",
      "x: 5524, Probs: [0.9941310286521912, 0.005868898238986731]\n",
      "x: 7425, Probs: [0.9995588660240173, 0.00044113959302194417]\n",
      "x: 88, Probs: [0.9992234706878662, 0.000776510511059314]\n",
      "x: 5572, Probs: [0.9945007562637329, 0.005499234888702631]\n",
      "x: 272, Probs: [0.9966050386428833, 0.003394993720576167]\n",
      "x: 4245, Probs: [0.9673061966896057, 0.03269381821155548]\n",
      "x: 6194, Probs: [0.9976389408111572, 0.0023610980715602636]\n",
      "x: 3154, Probs: [0.8697561025619507, 0.1302439421415329]\n",
      "x: 6844, Probs: [0.9990259408950806, 0.0009740187670104206]\n",
      "x: 3692, Probs: [0.9329398274421692, 0.06706010550260544]\n",
      "x: 5787, Probs: [0.9958935976028442, 0.004106420092284679]\n",
      "x: 3330, Probs: [0.8946227431297302, 0.10537729412317276]\n",
      "x: 873, Probs: [0.7015820741653442, 0.298417866230011]\n",
      "x: 8520, Probs: [0.9999009370803833, 9.908014180837199e-05]\n",
      "x: 9999, Probs: [0.9999867677688599, 1.3176523680158425e-05]\n",
      "x: 136, Probs: [0.9988586902618408, 0.0011413575848564506]\n",
      "x: 6391, Probs: [0.99819415807724, 0.0018058049026876688]\n",
      "x: 8877, Probs: [0.9999390840530396, 6.088012014515698e-05]\n",
      "x: 5958, Probs: [0.9967443943023682, 0.0032555891666561365]\n",
      "x: 7711, Probs: [0.9997013211250305, 0.00029867832199670374]\n",
      "x: 3063, Probs: [0.8550284504890442, 0.1449715495109558]\n",
      "x: 8746, Probs: [0.9999271631240845, 7.280974386958405e-05]\n",
      "x: 2315, Probs: [0.6800956726074219, 0.3199043571949005]\n",
      "x: 8319, Probs: [0.999869704246521, 0.00013032974675297737]\n",
      "x: 7268, Probs: [0.9994534850120544, 0.0005465421127155423]\n",
      "x: 6892, Probs: [0.9990874528884888, 0.0009124967036768794]\n",
      "x: 8358, Probs: [0.9998763799667358, 0.0001235754316439852]\n",
      "x: 5034, Probs: [0.9886119961738586, 0.0113879619166255]\n",
      "x: 3604, Probs: [0.925027072429657, 0.0749729722738266]\n",
      "x: 5441, Probs: [0.993431031703949, 0.006569027900695801]\n",
      "x: 4782, Probs: [0.9840131998062134, 0.015986846759915352]\n",
      "x: 4347, Probs: [0.971435546875, 0.028564468026161194]\n",
      "x: 4190, Probs: [0.9648468494415283, 0.03515312448143959]\n",
      "x: 7067, Probs: [0.9992813467979431, 0.0007186431321315467]\n",
      "x: 6605, Probs: [0.998650848865509, 0.001349145662970841]\n",
      "x: 3574, Probs: [0.9221411943435669, 0.07785877585411072]\n",
      "x: 6516, Probs: [0.9984766840934753, 0.0015233003068715334]\n",
      "x: 9826, Probs: [0.9999833106994629, 1.6676975064910948e-05]\n",
      "x: 2571, Probs: [0.7509182095527649, 0.24908174574375153]\n",
      "x: 145, Probs: [0.9987731575965881, 0.0012268194695934653]\n",
      "x: 3170, Probs: [0.8722045421600342, 0.12779542803764343]\n",
      "x: 3326, Probs: [0.8941095471382141, 0.10589049011468887]\n",
      "x: 7412, Probs: [0.9995511174201965, 0.00044890426215715706]\n",
      "x: 2337, Probs: [0.6866064667701721, 0.3133935034275055]\n",
      "x: 7487, Probs: [0.9995946288108826, 0.0004053708980791271]\n",
      "x: 9156, Probs: [0.9999583959579468, 4.1608760511735454e-05]\n",
      "x: 3885, Probs: [0.9476451277732849, 0.05235487222671509]\n",
      "x: 2759, Probs: [0.7957375049591064, 0.20426252484321594]\n",
      "x: 8728, Probs: [0.9999253749847412, 7.460907363565639e-05]\n",
      "x: 3434, Probs: [0.9072660207748413, 0.09273398667573929]\n",
      "x: 4199, Probs: [0.9652544856071472, 0.034745533019304276]\n",
      "x: 6373, Probs: [0.9981498718261719, 0.0018501264275982976]\n",
      "x: 8557, Probs: [0.9999058246612549, 9.419779962627217e-05]\n",
      "x: 5131, Probs: [0.9900082349777222, 0.009991812519729137]\n",
      "x: 4757, Probs: [0.983472466468811, 0.0165275726467371]\n",
      "x: 5183, Probs: [0.9906858801841736, 0.009314080700278282]\n",
      "x: 7496, Probs: [0.9995995163917542, 0.00040050342795439065]\n",
      "x: 9857, Probs: [0.9999840259552002, 1.59911851369543e-05]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x,_ in negative_samples_test[:100]:\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        outputs = model(x)\n",
    "        # apply softmax to get class probabilities\n",
    "        predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "        print(f\"x: {int(x.item())}, Probs: {predicted.tolist()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test FP rate: 0.00300\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluate(test_dataloader, model)\n",
    "\n",
    "# compute model predictions for all universe items\n",
    "universe = list(range(0,usize))\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    #for x,_ in negative_samples_test[:100]\n",
    "    for x in universe:\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        outputs = model(x)\n",
    "        # apply softmax to get class probabilities\n",
    "        predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "        #print(f\"x: {x}, Logits: {outputs}, Predicted: {predicted}\")\n",
    "        predictions.append(predicted[1].item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the model predictions\n",
    "plt.plot(universe, predictions)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Learned Oracle')\n",
    "\n",
    "# log scale on x axis\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
