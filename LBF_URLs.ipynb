{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned Bloom Filter - Malicious URLs Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sympy as sp\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import xxhash\n",
    "import bitarray\n",
    "from pympler import asizeof\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoUniversalHashFamily:\n",
    "    def __init__(self, m, max_key):\n",
    "        self.m = m  # Size of the hash table\n",
    "        self.p = sp.nextprime(max_key)  # generate a large prime number, greater than any key\n",
    "        self.a = random.randint(1, self.p-1)  # Choose a randomly\n",
    "        self.b = random.randint(0, self.p-1)  # Choose b randomly\n",
    "\n",
    "    def hash(self, k):\n",
    "        return ((self.a * k + self.b) % self.p) % self.m\n",
    "    \n",
    "    def __call__(self, k):\n",
    "        return self.hash(k)\n",
    "\n",
    "\n",
    "class BloomStandard:\n",
    "    def __init__(self, S, m, k=None):\n",
    "        self.m = m\n",
    "        if k is None:\n",
    "            self.k = max(1,round((m/len(S)) * math.log(2)))  # optimal number of hash functions for a given m and n \n",
    "            print(f\"Optimal number of hash functions: {self.k}\")      \n",
    "        else:\n",
    "            self.k = k\n",
    "\n",
    "        self.B = bitarray.bitarray(m)\n",
    "        self.B.setall(0)  # initialize all bits to 0\n",
    "\n",
    "        # construct bit array\n",
    "        for key in S:\n",
    "            self.insert(key)\n",
    "\n",
    "        print(f\"Bloom filter constructed! Size: {self.m}, Number of hash functions: {self.k}\")\n",
    "\n",
    "\n",
    "    # insert new integer key into the bloom filter \n",
    "    def insert(self, key):\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            self.B[hash_val] = 1\n",
    "\n",
    "    # poerform membership query for the given key\n",
    "    def query(self, key):\n",
    "        q = [0]*self.k\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            q[i] = self.B[hash_val]\n",
    "        if 0 in q:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.B)\n",
    "\n",
    "\n",
    "# define pytroch dataset class for training\n",
    "class BloomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, positive_samples, negative_samples):\n",
    "        if positive_samples is None:\n",
    "            positive_samples = []\n",
    "        self.samples = positive_samples + negative_samples\n",
    "        random.shuffle(self.samples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        # convert to tensor\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# define neural network with two hidden layers\n",
    "class Oracle(torch.nn.Module):\n",
    "    def __init__(self, hidden_dims=10, dropout_rate=0.2):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.hidden_layer1 = torch.nn.Linear(1, hidden_dims)\n",
    "        self.hidden_layer2 = torch.nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        #self.batch_norm = torch.nn.BatchNorm1d(hidden_dims)\n",
    "        #self.leaky_relu = torch.nn.LeakyReLU(0.01)\n",
    "        self.output_layer = torch.nn.Linear(hidden_dims, 2)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.tanh(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        #x = self.leaky_relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(train_dataloader, model, optimizer, num_epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "        num_correct, num_pos_correct, num_neg_correct = 0, 0, 0\n",
    "        avg_loss = 0\n",
    "        for batch in pbar:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            # flatten the labels\n",
    "            y = y.view(-1)      \n",
    "            loss = F.cross_entropy(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute moving average loss\n",
    "            avg_loss = 0.9 * avg_loss + 0.1 * loss.item()\n",
    "\n",
    "            # compute accuracy for batch precictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            accuracy = num_correct / num_total\n",
    "\n",
    "            # compute accuracy for positive samples\n",
    "            num_pos_total += y[y == 1].shape[0]\n",
    "            num_pos_correct += (predicted[y == 1] == 1).sum().item()\n",
    "            pos_accuracy = num_pos_correct / max(1,num_pos_total)\n",
    "\n",
    "            # compute accuracy for negative samples\n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "            num_neg_correct += (predicted[y == 0] == 0).sum().item()\n",
    "            neg_accuracy = num_neg_correct / max(1,num_neg_total)  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.5f}, Train Accuracy Overall: {accuracy: .5f}, Train Accuracy Positive: {pos_accuracy:.5f}, Train Accuracy Negative: {neg_accuracy:.5f}\")\n",
    "\n",
    "\n",
    "# evaluation on test samples\n",
    "def evaluate(test_dataloader, model, tau=0.5, device='cpu'):\n",
    "    num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "    num_correct = 0\n",
    "    num_FP, num_FN = 0, 0\n",
    "    model.eval()\n",
    "    xy = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            predicted = (outputs[:, 1] >= tau).long()\n",
    "            \n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            num_pos_total += y[y == 1].shape[0]            \n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "\n",
    "            # compute number of false positives and false negatives\n",
    "            num_FP += (predicted[y == 0] == 1).sum().item()\n",
    "            num_FN += (predicted[y == 1] == 0).sum().item() \n",
    "\n",
    "            xy.extend(list(zip(x.cpu().numpy(), y.cpu().numpy(), predicted.cpu().numpy())))\n",
    "\n",
    "    print(f\"Num total: {num_total}, Num correct: {num_correct}, Num False Positives: {num_FP}, Num False Negatives: {num_FN}\")\n",
    "\n",
    "    accuracy = num_correct / num_total\n",
    "    FP_rate = num_FP / max(1,num_neg_total)\n",
    "    FN_rate = num_FN / max(1,num_pos_total)                 \n",
    "    print(f\"Test Accuracy: {accuracy}, Test FP rate: {FP_rate:.5f}, Test FN rate: {FN_rate:.5f}\")\n",
    "    return FP_rate, xy\n",
    "\n",
    "\n",
    "class LearnedBF:\n",
    "    def __init__(self, oracle, backup_bf, x_mean, x_std, tau=0.5, device='cuda'):\n",
    "        self.oracle = oracle\n",
    "        self.backup_bf = backup_bf\n",
    "        self.tau = tau\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.device = device\n",
    "\n",
    "    def query(self, key):\n",
    "        if self.oracle_predict(key):\n",
    "            return True\n",
    "        else:\n",
    "            return self.backup_bf.query(key)\n",
    "\n",
    "    def oracle_predict(self, x):\n",
    "        x = (key - self.x_mean) / self.x_std\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        outputs = self.oracle(x)\n",
    "        predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "        return predicted[1].item() > self.tau\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'datasets/URLs_Singh/Webpages_Classification_train_data.csv/Webpages_Classification_train_data.csv'\n",
    "\n",
    "# load only 'url' and 'label' columns\n",
    "columns_to_load = [1, 11]\n",
    "\n",
    "# Initialize empty lists to hold the data of the two columns\n",
    "urls = []\n",
    "labels = []\n",
    "\n",
    "# Open the CSV file and read line by line\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        # Append the data from the specified columns to the lists\n",
    "        urls.append(row[columns_to_load[0]])\n",
    "        labels.append(row[columns_to_load[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good URLs: 1172747, Number of bad URLs: 27253\n"
     ]
    }
   ],
   "source": [
    "# separate the data into good and bad URLs\n",
    "good_urls = [url for url, label in zip(urls, labels) if label == 'good']\n",
    "bad_urls = [url for url, label in zip(urls, labels) if label == 'bad']\n",
    "\n",
    "print(f\"Number of good URLs: {len(good_urls)}, Number of bad URLs: {len(bad_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://members.tripod.com/russiastation/ --> good\n",
      "http://www.ddj.com/cpp/184403822 --> good\n",
      "http://www.naef-usa.com/ --> good\n",
      "http://www.ff-b2b.de/ --> bad\n",
      "http://us.imdb.com/title/tt0176269/ --> good\n",
      "http://efilmcritic.com/hbs.cgi?movie=311 --> good\n",
      "http://christian.net/ --> good\n",
      "http://www.indsource.com --> good\n",
      "http://www.greatestescapes.com --> good\n"
     ]
    }
   ],
   "source": [
    "# show some example instances\n",
    "for i in range(1,10):\n",
    "    print(f\"{urls[i]} --> {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of hash functions: 3\n",
      "Bloom filter constructed! Size: 109012, Number of hash functions: 3\n"
     ]
    }
   ],
   "source": [
    "# construct a bloom filter on the bad URLs\n",
    "bf = BloomStandard(S=bad_urls, m=4*len(bad_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.1534\n",
      "Memory usage for bad URLs: 2662.504 kilobytes, Memory usage for Bloom filter: 13.712 kilobytes\n"
     ]
    }
   ],
   "source": [
    "# draw some random negative samples from the good URLs\n",
    "neg_samples = random.sample(good_urls, 5000)\n",
    "\n",
    "# evaluate the bloom filter on the negative samples\n",
    "FP_rate = sum([bf.query(url) for url in neg_samples])/len(neg_samples)\n",
    "print(f\"False Positive Rate: {FP_rate}\")\n",
    "\n",
    "\n",
    "print(f\"Memory usage for bad URLs: {asizeof.asizeof(bad_urls)/1000} kilobytes, Memory usage for Bloom filter: {asizeof.asizeof(bf.B)/1000} kilobytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
