{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned Bloom Filter - Malicious URLs Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sympy as sp\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import xxhash\n",
    "import bitarray\n",
    "from pympler import asizeof\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good URLs: 344799, Number of bad URLs: 66385\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#file_path = 'datasets/URLs_Singh/Webpages_Classification_train_data.csv/Webpages_Classification_train_data.csv'\n",
    "file_path = 'datasets/AdaBF_URL_data.csv'\n",
    "\n",
    "# load only 'url' and 'label' columns\n",
    "columns_to_load = [0,1] #[1, 11]\n",
    "\n",
    "# Initialize empty lists to hold the data of the two columns\n",
    "urls = []\n",
    "labels = []\n",
    "\n",
    "# Open the CSV file and read line by line\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader) # skip header\n",
    "    for row in reader:\n",
    "        # Append the data from the specified columns to the lists\n",
    "        urls.append(row[columns_to_load[0]])\n",
    "        labels.append(row[columns_to_load[1]])\n",
    "\n",
    "# separate the data into good and bad URLs\n",
    "benign_urls = [url for url, label in zip(urls, labels) if label == '-1']\n",
    "malicious_urls = [url for url, label in zip(urls, labels) if label == '1']\n",
    "\n",
    "print(f\"Number of good URLs: {len(benign_urls)}, Number of bad URLs: {len(malicious_urls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bening URLs:\n",
      "monsterzine.com/200301/kingkong.php\n",
      "budiz.com/socialnetworks/arts/drawing/fashion-illustration-social-network\n",
      "disney.go.com/home/html/index.html\n",
      "waatp.com/people/erin-everly/658742/\n",
      "\n",
      "Malicious URLs:\n",
      "congressomossoroense.com.br/nophfjkgjfshgjdfhjfhkj/\n",
      "oweridreamsact.com.ng/_vti_009/serverphp/cp.php?m=login\n",
      "ranhadinhen.ru/gate.php\n",
      "sie-liebt-mich.de/yuilk/djchsj/03a7028413fb70343d6344476ad6a5b1/\n"
     ]
    }
   ],
   "source": [
    "# show some example instances\n",
    "print(f\"Bening URLs:\")\n",
    "for i in range(1,5):\n",
    "    print(f\"{benign_urls[i]}\")\n",
    "\n",
    "print(f\"\\nMalicious URLs:\")\n",
    "for i in range(1,5):\n",
    "    print(f\"{malicious_urls[i]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoUniversalHashFamily:\n",
    "    def __init__(self, m, max_key):\n",
    "        self.m = m  # Size of the hash table\n",
    "        self.p = sp.nextprime(max_key)  # generate a large prime number, greater than any key\n",
    "        self.a = random.randint(1, self.p-1)  # Choose a randomly\n",
    "        self.b = random.randint(0, self.p-1)  # Choose b randomly\n",
    "\n",
    "    def hash(self, k):\n",
    "        return ((self.a * k + self.b) % self.p) % self.m\n",
    "    \n",
    "    def __call__(self, k):\n",
    "        return self.hash(k)\n",
    "\n",
    "\n",
    "class BloomStandard:\n",
    "    def __init__(self, S, m, k=None):\n",
    "        self.m = m\n",
    "        if k is None:\n",
    "            self.k = max(1,round((m/len(S)) * math.log(2)))  # optimal number of hash functions for a given m and n \n",
    "            print(f\"Optimal number of hash functions: {self.k}\")      \n",
    "        else:\n",
    "            self.k = k\n",
    "\n",
    "        self.B = bitarray.bitarray(m)\n",
    "        self.B.setall(0)  # initialize all bits to 0\n",
    "\n",
    "        # construct bit array\n",
    "        for key in S:\n",
    "            self.insert(key)\n",
    "\n",
    "        print(f\"Bloom filter constructed! Size: {self.m}, Number of hash functions: {self.k}\")\n",
    "\n",
    "\n",
    "    # insert new integer key into the bloom filter \n",
    "    def insert(self, key):\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            self.B[hash_val] = 1\n",
    "\n",
    "    # poerform membership query for the given key\n",
    "    def query(self, key):\n",
    "        q = [0]*self.k\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            q[i] = self.B[hash_val]\n",
    "        if 0 in q:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.B)\n",
    "\n",
    "\n",
    "# define pytroch dataset class for training\n",
    "class URLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, urls, labels):\n",
    "        self.samples = list(zip(urls, labels))\n",
    "        random.shuffle(self.samples)\n",
    "        self.label2idx = {'malicious': 1, 'benign': 0}\n",
    "        # character vocabulary\n",
    "        self.vocab = ['<PAD>'] + list(set(''.join(urls))) \n",
    "        self.char2idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        url, label = self.samples[idx]\n",
    "        # tokenize the url\n",
    "        x = [self.char2idx[char] for char in url]\n",
    "        y = self.label2idx[label]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Unzip the batch to separate sequences and labels\n",
    "    sequences, labels = zip(*batch)\n",
    "    # Convert sequences to tensors and pad\n",
    "    sequences = [torch.tensor(sequence, dtype=torch.long) for sequence in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)  # padding index is 0\n",
    "    # Convert labels to a tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return padded_sequences, labels\n",
    "\n",
    "\n",
    "# define stacked RNN classifier oracle\n",
    "class Oracle(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims=16, hidden_dims=8, num_layers=2, dropout_rate=0.2, padding_idx=0):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dims)\n",
    "        # 2 stacked RNN GRU layers\n",
    "        self.rnn = torch.nn.GRU(embedding_dims, hidden_dims, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)    \n",
    "        self.output_layer = torch.nn.Linear(hidden_dims*2, 2)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embedding(x) # shape: (batch_size, seq_len, embedding_dims)\n",
    "        x = self.dropout(x) # shape: (batch_size, seq_len, embedding_dims)\n",
    "        # get RNN final hidden states\n",
    "        _, h = self.rnn(x) # shape: (num_layers*2, batch_size, hidden_dims*2)\n",
    "        # concatenate the final forward and backward hidden states of the last layer\n",
    "        h_forward = h[-2, :, :]  # Last layer forward hidden state\n",
    "        h_backward = h[-1, :, :]  # Last layer backward hidden state\n",
    "        x = torch.cat((h_forward, h_backward), dim=1)  # shape: (batch_size, hidden_dims*2)\n",
    "        # map to output layer\n",
    "        x = self.output_layer(x) # shape: (batch_size, 2)\n",
    "        if y is not None:\n",
    "            y = y.view(-1)\n",
    "            loss = F.cross_entropy(x, y)\n",
    "            return x, loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(train_dataloader, val_dataloader, model, optimizer, num_epochs=10, val_every=1, device='cpu'):\n",
    "    model.train()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "        num_correct, num_pos_correct, num_neg_correct = 0, 0, 0\n",
    "        avg_loss = 0\n",
    "        for batch in pbar:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_logits, loss = model(x, y)           \n",
    "            loss.backward()\n",
    "            # clip gradients to avoid exploding gradients\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute moving average loss\n",
    "            avg_loss = 0.9 * avg_loss + 0.1 * loss.item()\n",
    "\n",
    "            # compute accuracy for batch precictions\n",
    "            _, predicted = torch.max(output_logits, 1)\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            accuracy = num_correct / num_total\n",
    "\n",
    "            # compute accuracy for positive samples\n",
    "            num_pos_total += y[y == 1].shape[0]\n",
    "            num_pos_correct += (predicted[y == 1] == 1).sum().item()\n",
    "            pos_accuracy = num_pos_correct / max(1,num_pos_total)\n",
    "\n",
    "            # compute accuracy for negative samples\n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "            num_neg_correct += (predicted[y == 0] == 0).sum().item()\n",
    "            neg_accuracy = num_neg_correct / max(1,num_neg_total)  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.5f}, Train Accuracy Overall: {accuracy: .5f}, Train Accuracy Positive: {pos_accuracy:.5f}, Train Accuracy Negative: {neg_accuracy:.5f}, Val Loss: {val_loss:.5f}, Val Accuracy: {val_accuracy:.5f}\")\n",
    "\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            val_loss, val_accuracy = evaluate(val_dataloader, model, device=device)\n",
    "\n",
    "# evaluation on test samples\n",
    "def evaluate(test_dataloader, model, tau=0.5, device='cpu', verbose=False):\n",
    "    num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "    num_correct = 0\n",
    "    num_FP, num_FN = 0, 0\n",
    "    avg_loss = 0.0\n",
    "    model.eval()\n",
    "    #xy = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output_logits, loss = model(x, y)\n",
    "            avg_loss += loss.item()\n",
    "            positive_probs = torch.nn.Softmax(dim=1)(output_logits)[:, 1]\n",
    "            predicted = (positive_probs >= tau).long()\n",
    "            \n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            num_pos_total += y[y == 1].shape[0]            \n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "\n",
    "            # compute number of false positives and false negatives\n",
    "            num_FP += (predicted[y == 0] == 1).sum().item()\n",
    "            num_FN += (predicted[y == 1] == 0).sum().item() \n",
    "\n",
    "            #xy.extend(list(zip(x.cpu().numpy(), y.cpu().numpy(), predicted.cpu().numpy())))\n",
    "    model.train()\n",
    "\n",
    "    avg_loss /= len(test_dataloader)\n",
    "    accuracy = num_correct / num_total\n",
    "\n",
    "    if verbose:\n",
    "        FP_rate = num_FP / max(1,num_neg_total)\n",
    "        FN_rate = num_FN / max(1,num_pos_total)                 \n",
    "        print(f\"Num total: {num_total}, Num correct: {num_correct}, Num False Positives: {num_FP}, Num False Negatives: {num_FN}\")\n",
    "        print(f\"Test Accuracy: {accuracy}, Test FP rate: {FP_rate:.5f}, Test FN rate: {FN_rate:.5f}\")\n",
    "    return  avg_loss, accuracy #, xy\n",
    "\n",
    "\n",
    "class LearnedBF:\n",
    "    def __init__(self, oracle, backup_bf, x_mean, x_std, tau=0.5, device='cuda'):\n",
    "        self.oracle = oracle\n",
    "        self.backup_bf = backup_bf\n",
    "        self.tau = tau\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.device = device\n",
    "\n",
    "    def query(self, key):\n",
    "        if self.oracle_predict(key):\n",
    "            return True\n",
    "        else:\n",
    "            return self.backup_bf.query(key)\n",
    "\n",
    "    def oracle_predict(self, x):\n",
    "        x = (key - self.x_mean) / self.x_std\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        outputs = self.oracle(x)\n",
    "        predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "        return predicted[1].item() > self.tau\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the standard bloom filter on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of hash functions: 3\n",
      "Bloom filter constructed! Size: 265540, Number of hash functions: 3\n",
      "False Positive Rate: 0.1434\n",
      "Memory usage for bad URLs: 7754.68 kilobytes, Memory usage for Bloom filter: 33.28 kilobytes\n"
     ]
    }
   ],
   "source": [
    "# construct a bloom filter on the bad URLs\n",
    "bf = BloomStandard(S=malicious_urls, m=4*len(malicious_urls))\n",
    "\n",
    "# draw some random negative samples from the good URLs\n",
    "neg_samples = random.sample(benign_urls, 5000)\n",
    "\n",
    "# evaluate the bloom filter on the negative samples\n",
    "FP_rate = sum([bf.query(url) for url in neg_samples])/len(neg_samples)\n",
    "print(f\"False Positive Rate: {FP_rate}\")\n",
    "\n",
    "\n",
    "print(f\"Memory usage for bad URLs: {asizeof.asizeof(malicious_urls)/1000} kilobytes, Memory usage for Bloom filter: {asizeof.asizeof(bf.B)/1000} kilobytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train an oracle on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310320"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66385\n",
      "Vocabulary size: 177\n",
      "Vocabulary size: 101\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# hold out 5% of good URLs for testing\n",
    "test_size = int(0.1*len(benign_urls))\n",
    "benign_urls_train = benign_urls[:-test_size]\n",
    "benign_urls_test = benign_urls[-test_size:]\n",
    "\n",
    "urls_train = benign_urls_train + malicious_urls\n",
    "labels_train = ['benign']*len(benign_urls_train) + ['malicious']*len(malicious_urls)\n",
    "urls_test = benign_urls_test\n",
    "labels_test = ['benign']*len(benign_urls_test)\n",
    "\n",
    "print(len(benign_urls_train), len(malicious_urls))\n",
    "\n",
    "train_dataset = URLDataset(urls_train, labels_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True, pin_memory=False)\n",
    "\n",
    "test_dataset = URLDataset(urls_test, labels_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanzid/miniconda3/envs/torch_clone_2/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# model hyperparameters\n",
    "embedding_dims = 16\n",
    "hidden_dims = 8\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "# define model and optimizer\n",
    "model = Oracle(vocab_size=train_dataset.vocab_size, embedding_dims=embedding_dims, hidden_dims=hidden_dims, dropout_rate=0.1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.55159, Train Accuracy Overall:  0.82377, Train Accuracy Positive: 0.00000, Train Accuracy Negative: 1.00000, Val Loss: 0.00000, Val Accuracy: 0.00000: 100%|██████████| 5887/5887 [00:47<00:00, 123.74it/s]\n",
      "Epoch 2, Train Loss: 0.53621, Train Accuracy Overall:  0.82377, Train Accuracy Positive: 0.00000, Train Accuracy Negative: 1.00000, Val Loss: 0.20676, Val Accuracy: 1.00000: 100%|██████████| 5887/5887 [00:46<00:00, 125.70it/s]\n",
      "Epoch 3, Train Loss: 0.42260, Train Accuracy Overall:  0.82378, Train Accuracy Positive: 0.00002, Train Accuracy Negative: 1.00000, Val Loss: 0.21247, Val Accuracy: 1.00000: 100%|██████████| 5887/5887 [00:44<00:00, 132.06it/s]\n",
      "Epoch 4, Train Loss: 0.49840, Train Accuracy Overall:  0.82381, Train Accuracy Positive: 0.00023, Train Accuracy Negative: 0.99999, Val Loss: 0.22349, Val Accuracy: 1.00000: 100%|██████████| 5887/5887 [00:45<00:00, 129.12it/s]\n",
      "Epoch 5, Train Loss: 0.45127, Train Accuracy Overall:  0.82426, Train Accuracy Positive: 0.00301, Train Accuracy Negative: 0.99994, Val Loss: 0.25333, Val Accuracy: 0.99930: 100%|██████████| 5887/5887 [00:44<00:00, 133.58it/s]\n",
      "Epoch 6, Train Loss: 0.43787, Train Accuracy Overall:  0.82958, Train Accuracy Positive: 0.03520, Train Accuracy Negative: 0.99952, Val Loss: 0.29829, Val Accuracy: 0.99577: 100%|██████████| 5887/5887 [00:47<00:00, 124.18it/s]\n",
      "Epoch 7, Train Loss: 0.45726, Train Accuracy Overall:  0.83574, Train Accuracy Positive: 0.07553, Train Accuracy Negative: 0.99837, Val Loss: 0.36504, Val Accuracy: 0.95296: 100%|██████████| 5887/5887 [00:43<00:00, 135.11it/s]\n",
      "Epoch 8, Train Loss: 0.42045, Train Accuracy Overall:  0.83871, Train Accuracy Positive: 0.10203, Train Accuracy Negative: 0.99631, Val Loss: 0.44424, Val Accuracy: 0.84930: 100%|██████████| 5887/5887 [00:45<00:00, 128.37it/s]\n",
      "Epoch 9, Train Loss: 0.41568, Train Accuracy Overall:  0.84095, Train Accuracy Positive: 0.12611, Train Accuracy Negative: 0.99387, Val Loss: 0.53896, Val Accuracy: 0.70568: 100%|██████████| 5887/5887 [00:44<00:00, 131.28it/s]\n",
      "Epoch 10, Train Loss: 0.38958, Train Accuracy Overall:  0.84261, Train Accuracy Positive: 0.14582, Train Accuracy Negative: 0.99167, Val Loss: 0.62120, Val Accuracy: 0.56864: 100%|██████████| 5887/5887 [00:42<00:00, 138.75it/s]\n",
      "Epoch 11, Train Loss: 0.40002, Train Accuracy Overall:  0.84398, Train Accuracy Positive: 0.16385, Train Accuracy Negative: 0.98948, Val Loss: 0.70024, Val Accuracy: 0.50080: 100%|██████████| 5887/5887 [00:43<00:00, 135.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# train oracle\n",
    "train(train_dataloader, test_dataloader, model, optimizer, num_epochs=20, val_every=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 101\n"
     ]
    }
   ],
   "source": [
    "urls_test = good_urls_test\n",
    "labels_test = ['good']*len(good_urls_test)\n",
    "test_dataset = URLDataset(urls_test, labels_test)\n",
    "\n",
    "# Create a DataLoader with the custom collate function\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num total: 34479, Num correct: 471, Num False Positives: 34008, Num False Negatives: 0\n",
      "Test Accuracy: 0.01366048899330027, Test FP rate: 0.98634, Test FN rate: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "FP_rate, xy = evaluate(test_dataloader, model, tau=0.5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num total: 376705, Num correct: 357574, Num False Positives: 2007, Num False Negatives: 17124\n",
      "Test Accuracy: 0.9492149029081112, Test FP rate: 0.00647, Test FN rate: 0.25795\n"
     ]
    }
   ],
   "source": [
    "# evaluate on training set\n",
    "FP_rate, xy = evaluate(train_dataloader, model, tau=0.5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
