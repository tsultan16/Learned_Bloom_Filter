{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned Bloom Filter - Malicious URLs Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sympy as sp\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import xxhash\n",
    "import bitarray\n",
    "from pympler import asizeof\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good URLs: 344799, Number of bad URLs: 66385\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "#file_path = 'datasets/URLs_Singh/Webpages_Classification_train_data.csv/Webpages_Classification_train_data.csv'\n",
    "file_path = 'datasets/AdaBF_URL_data.csv'\n",
    "\n",
    "# load only 'url' and 'label' columns\n",
    "columns_to_load = [0,1] #[1, 11]\n",
    "\n",
    "# Initialize empty lists to hold the data of the two columns\n",
    "urls = []\n",
    "labels = []\n",
    "\n",
    "# Open the CSV file and read line by line\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader) # skip header\n",
    "    for row in reader:\n",
    "        # Append the data from the specified columns to the lists\n",
    "        urls.append(row[columns_to_load[0]])\n",
    "        labels.append(row[columns_to_load[1]])\n",
    "\n",
    "# separate the data into good and bad URLs\n",
    "benign_urls = [url for url, label in zip(urls, labels) if label == '-1']\n",
    "malicious_urls = [url for url, label in zip(urls, labels) if label == '1']\n",
    "\n",
    "print(f\"Number of good URLs: {len(benign_urls)}, Number of bad URLs: {len(malicious_urls)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bening URLs:\n",
      "monsterzine.com/200301/kingkong.php\n",
      "budiz.com/socialnetworks/arts/drawing/fashion-illustration-social-network\n",
      "disney.go.com/home/html/index.html\n",
      "waatp.com/people/erin-everly/658742/\n",
      "\n",
      "Malicious URLs:\n",
      "congressomossoroense.com.br/nophfjkgjfshgjdfhjfhkj/\n",
      "oweridreamsact.com.ng/_vti_009/serverphp/cp.php?m=login\n",
      "ranhadinhen.ru/gate.php\n",
      "sie-liebt-mich.de/yuilk/djchsj/03a7028413fb70343d6344476ad6a5b1/\n"
     ]
    }
   ],
   "source": [
    "# show some example instances\n",
    "print(f\"Bening URLs:\")\n",
    "for i in range(1,5):\n",
    "    print(f\"{benign_urls[i]}\")\n",
    "\n",
    "print(f\"\\nMalicious URLs:\")\n",
    "for i in range(1,5):\n",
    "    print(f\"{malicious_urls[i]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoUniversalHashFamily:\n",
    "    def __init__(self, m, max_key):\n",
    "        self.m = m  # Size of the hash table\n",
    "        self.p = sp.nextprime(max_key)  # generate a large prime number, greater than any key\n",
    "        self.a = random.randint(1, self.p-1)  # Choose a randomly\n",
    "        self.b = random.randint(0, self.p-1)  # Choose b randomly\n",
    "\n",
    "    def hash(self, k):\n",
    "        return ((self.a * k + self.b) % self.p) % self.m\n",
    "    \n",
    "    def __call__(self, k):\n",
    "        return self.hash(k)\n",
    "\n",
    "\n",
    "class BloomStandard:\n",
    "    def __init__(self, S, m, k=None):\n",
    "        self.m = m\n",
    "        if k is None:\n",
    "            self.k = max(1,round((m/len(S)) * math.log(2)))  # optimal number of hash functions for a given m and n \n",
    "            print(f\"Optimal number of hash functions: {self.k}\")      \n",
    "        else:\n",
    "            self.k = k\n",
    "\n",
    "        self.B = bitarray.bitarray(m)\n",
    "        self.B.setall(0)  # initialize all bits to 0\n",
    "\n",
    "        # construct bit array\n",
    "        for key in S:\n",
    "            self.insert(key)\n",
    "\n",
    "        print(f\"Bloom filter constructed! Size: {self.m}, Number of hash functions: {self.k}\")\n",
    "\n",
    "\n",
    "    # insert new integer key into the bloom filter \n",
    "    def insert(self, key):\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            self.B[hash_val] = 1\n",
    "\n",
    "    # poerform membership query for the given key\n",
    "    def query(self, key):\n",
    "        q = [0]*self.k\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            q[i] = self.B[hash_val]\n",
    "        if 0 in q:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.B)\n",
    "\n",
    "\n",
    "# define pytroch dataset class for training\n",
    "class URLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, urls, labels):\n",
    "        self.samples = list(zip(urls, labels))\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Unzip the batch to separate sequences and labels\n",
    "    sequences, labels = zip(*batch)\n",
    "    # Convert sequences to tensors and pad\n",
    "    sequences = [torch.tensor(sequence, dtype=torch.long) for sequence in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)  # padding index is 0\n",
    "    # Convert labels to a tensor\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return padded_sequences, labels\n",
    "\n",
    "\n",
    "# define stacked RNN classifier oracle\n",
    "class Oracle(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims=16, hidden_dims=8, num_layers=2, dropout_rate=0.2, padding_idx=0):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dims)\n",
    "        # 2 stacked RNN GRU layers\n",
    "        self.rnn = torch.nn.GRU(embedding_dims, hidden_dims, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)    \n",
    "        self.output_layer = torch.nn.Linear(hidden_dims*2, 2)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embedding(x) # shape: (batch_size, seq_len, embedding_dims)\n",
    "        x = self.dropout(x) # shape: (batch_size, seq_len, embedding_dims)\n",
    "        # get RNN final hidden states\n",
    "        _, h = self.rnn(x) # shape: (num_layers*2, batch_size, hidden_dims*2)\n",
    "        # concatenate the final forward and backward hidden states of the last layer\n",
    "        h_forward = h[-2, :, :]  # Last layer forward hidden state\n",
    "        h_backward = h[-1, :, :]  # Last layer backward hidden state\n",
    "        x = torch.cat((h_forward, h_backward), dim=1)  # shape: (batch_size, hidden_dims*2)\n",
    "        # map to output layer\n",
    "        x = self.output_layer(x) # shape: (batch_size, 2)\n",
    "        if y is not None:\n",
    "            y = y.view(-1)\n",
    "            loss = F.cross_entropy(x, y)\n",
    "            return x, loss\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(train_dataloader, val_dataloader, model, optimizer, num_epochs=10, val_every=1, device='cpu'):\n",
    "    model.train()\n",
    "    val_loss, val_accuracy = 0, 0\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "        num_correct, num_pos_correct, num_neg_correct = 0, 0, 0\n",
    "        avg_loss = 0\n",
    "        for batch in pbar:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output_logits, loss = model(x, y)           \n",
    "            loss.backward()\n",
    "            # clip gradients to avoid exploding gradients\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute moving average loss\n",
    "            avg_loss = 0.9 * avg_loss + 0.1 * loss.item()\n",
    "\n",
    "            # compute accuracy for batch precictions\n",
    "            _, predicted = torch.max(output_logits, 1)\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            accuracy = num_correct / num_total\n",
    "\n",
    "            # compute accuracy for positive samples\n",
    "            num_pos_total += y[y == 1].shape[0]\n",
    "            num_pos_correct += (predicted[y == 1] == 1).sum().item()\n",
    "            pos_accuracy = num_pos_correct / max(1,num_pos_total)\n",
    "\n",
    "            # compute accuracy for negative samples\n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "            num_neg_correct += (predicted[y == 0] == 0).sum().item()\n",
    "            neg_accuracy = num_neg_correct / max(1,num_neg_total)  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.5f}, Train Accuracy Overall: {accuracy: .5f}, Train Accuracy Positive: {pos_accuracy:.5f}, Train Accuracy Negative: {neg_accuracy:.5f}, Val Loss: {val_loss:.5f}, Val Accuracy: {val_accuracy:.5f}\")\n",
    "\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            val_loss, val_accuracy = evaluate(val_dataloader, model, device=device)\n",
    "\n",
    "# evaluation on test samples\n",
    "def evaluate(test_dataloader, model, tau=0.5, device='cpu', verbose=False, return_xy=False):\n",
    "    num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "    num_correct = 0\n",
    "    num_FP, num_FN = 0, 0\n",
    "    avg_loss = 0.0\n",
    "    model.eval()\n",
    "    xy = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output_logits, loss = model(x, y)\n",
    "            avg_loss += loss.item()\n",
    "            positive_probs = torch.nn.Softmax(dim=1)(output_logits)[:, 1]\n",
    "            predicted = (positive_probs >= tau).long()\n",
    "            \n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            num_pos_total += y[y == 1].shape[0]            \n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "\n",
    "            # compute number of false positives and false negatives\n",
    "            num_FP += (predicted[y == 0] == 1).sum().item()\n",
    "            num_FN += (predicted[y == 1] == 0).sum().item() \n",
    "             \n",
    "            if return_xy: xy.extend(list(zip(x.cpu().numpy(), y.cpu().numpy(), predicted.cpu().numpy())))\n",
    "    model.train()\n",
    "    avg_loss /= len(test_dataloader)\n",
    "    accuracy = num_correct / num_total\n",
    "\n",
    "    if verbose:\n",
    "        FP_rate = num_FP / max(1,num_neg_total)\n",
    "        FN_rate = num_FN / max(1,num_pos_total)                 \n",
    "        print(f\"Num total: {num_total}, Num correct: {num_correct}, Num False Positives: {num_FP}, Num False Negatives: {num_FN}\")\n",
    "        print(f\"Test Accuracy: {accuracy}, Test FP rate: {FP_rate:.5f}, Test FN rate: {FN_rate:.5f}\")\n",
    "\n",
    "    if return_xy:\n",
    "        return xy    \n",
    "    return  avg_loss, accuracy #, xy\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the standard bloom filter on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of hash functions: 3\n",
      "Bloom filter constructed! Size: 265540, Number of hash functions: 3\n",
      "False Positive Rate: 0.1434\n",
      "Memory usage for bad URLs: 7754.68 kilobytes, Memory usage for Bloom filter: 33.28 kilobytes\n"
     ]
    }
   ],
   "source": [
    "# construct a bloom filter on the bad URLs\n",
    "bf = BloomStandard(S=malicious_urls, m=4*len(malicious_urls))\n",
    "\n",
    "# draw some random negative samples from the good URLs\n",
    "neg_samples = random.sample(benign_urls, 5000)\n",
    "\n",
    "# evaluate the bloom filter on the negative samples\n",
    "FP_rate = sum([bf.query(url) for url in neg_samples])/len(neg_samples)\n",
    "print(f\"False Positive Rate: {FP_rate}\")\n",
    "\n",
    "\n",
    "print(f\"Memory usage for bad URLs: {asizeof.asizeof(malicious_urls)/1000} kilobytes, Memory usage for Bloom filter: {asizeof.asizeof(bf.B)/1000} kilobytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train an oracle on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build character vocabulary\n",
    "vocab = ['<PAD>'] + list(set(''.join(urls)))\n",
    "vocab_size = len(vocab)\n",
    "char2idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "# tokenize the URLs\n",
    "benign_urls_tokenized = [[char2idx[char] for char in url] for url in benign_urls]\n",
    "malicious_urls_tokenized = [[char2idx[char] for char in url] for url in malicious_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327560 66385\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# hold out 5% of good URLs for testing\n",
    "test_size = int(0.05*len(benign_urls_tokenized))\n",
    "random.shuffle(benign_urls_tokenized)\n",
    "benign_urls_train = benign_urls_tokenized[:-test_size]\n",
    "benign_urls_test = benign_urls_tokenized[-test_size:]\n",
    "\n",
    "urls_train = benign_urls_train + malicious_urls_tokenized\n",
    "labels_train = [0]*len(benign_urls_train) + [1]*len(malicious_urls_tokenized)\n",
    "urls_test = benign_urls_test\n",
    "labels_test = [0]*len(benign_urls_test)\n",
    "\n",
    "print(len(benign_urls_train), len(malicious_urls_tokenized))\n",
    "\n",
    "train_dataset = URLDataset(urls_train, labels_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=True, pin_memory=False)\n",
    "\n",
    "test_dataset = URLDataset(urls_test, labels_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=custom_collate_fn, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanzid/miniconda3/envs/torch_clone_2/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# model hyperparameters\n",
    "embedding_dims = 32\n",
    "hidden_dims = 16\n",
    "device = 'cuda'\n",
    "\n",
    "# define model and optimizer\n",
    "model = Oracle(vocab_size=vocab_size, embedding_dims=embedding_dims, hidden_dims=hidden_dims, dropout_rate=0.1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.12093, Train Accuracy Overall:  0.95237, Train Accuracy Positive: 0.80874, Train Accuracy Negative: 0.98148, Val Loss: 0.00000, Val Accuracy: 0.00000: 100%|██████████| 6156/6156 [00:39<00:00, 155.45it/s]\n",
      "Epoch 2, Train Loss: 0.11282, Train Accuracy Overall:  0.95220, Train Accuracy Positive: 0.80932, Train Accuracy Negative: 0.98116, Val Loss: 0.00000, Val Accuracy: 0.00000: 100%|██████████| 6156/6156 [00:42<00:00, 146.11it/s]\n",
      "Epoch 3, Train Loss: 0.18232, Train Accuracy Overall:  0.95276, Train Accuracy Positive: 0.81074, Train Accuracy Negative: 0.98154, Val Loss: 0.03991, Val Accuracy: 0.98712: 100%|██████████| 6156/6156 [00:45<00:00, 136.02it/s]\n",
      "Epoch 4, Train Loss: 0.13284, Train Accuracy Overall:  0.95277, Train Accuracy Positive: 0.81154, Train Accuracy Negative: 0.98140, Val Loss: 0.03991, Val Accuracy: 0.98712: 100%|██████████| 6156/6156 [00:46<00:00, 132.79it/s]\n",
      "Epoch 5, Train Loss: 0.14627, Train Accuracy Overall:  0.95345, Train Accuracy Positive: 0.81421, Train Accuracy Negative: 0.98166, Val Loss: 0.03893, Val Accuracy: 0.98741: 100%|██████████| 6156/6156 [00:46<00:00, 131.14it/s]\n",
      "Epoch 6, Train Loss: 0.13286, Train Accuracy Overall:  0.95343, Train Accuracy Positive: 0.81422, Train Accuracy Negative: 0.98164, Val Loss: 0.03893, Val Accuracy: 0.98741: 100%|██████████| 6156/6156 [00:45<00:00, 135.56it/s]\n",
      "Epoch 7, Train Loss: 0.12703, Train Accuracy Overall:  0.95393, Train Accuracy Positive: 0.81607, Train Accuracy Negative: 0.98186, Val Loss: 0.04079, Val Accuracy: 0.98660: 100%|██████████| 6156/6156 [00:46<00:00, 132.24it/s]\n",
      "Epoch 8, Train Loss: 0.11114, Train Accuracy Overall:  0.95395, Train Accuracy Positive: 0.81764, Train Accuracy Negative: 0.98157, Val Loss: 0.04079, Val Accuracy: 0.98660: 100%|██████████| 6156/6156 [00:46<00:00, 131.60it/s]\n",
      "Epoch 9, Train Loss: 0.12040, Train Accuracy Overall:  0.95451, Train Accuracy Positive: 0.81832, Train Accuracy Negative: 0.98211, Val Loss: 0.03625, Val Accuracy: 0.98828: 100%|██████████| 6156/6156 [00:47<00:00, 130.42it/s]\n",
      "Epoch 10, Train Loss: 0.12652, Train Accuracy Overall:  0.95460, Train Accuracy Positive: 0.81875, Train Accuracy Negative: 0.98213, Val Loss: 0.03625, Val Accuracy: 0.98828: 100%|██████████| 6156/6156 [00:47<00:00, 128.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# train oracle\n",
    "train(train_dataloader, test_dataloader, model, optimizer, num_epochs=10, val_every=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model checkpoint for later use\n",
    "#torch.save(model.state_dict(), 'oracle_URL.pth')\n",
    "\n",
    "# load the model checkpoint\n",
    "#model.load_state_dict(torch.load('oracle_URL.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num total: 17239, Num correct: 16743, Num False Positives: 496, Num False Negatives: 0\n",
      "Test Accuracy: 0.9712280294680666, Test FP rate: 0.02877, Test FN rate: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "xy = evaluate(test_dataloader, model, tau=0.2, device=device, verbose=True, return_xy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num total: 393945, Num correct: 377314, Num False Positives: 9513, Num False Negatives: 7118\n",
      "Test Accuracy: 0.9577834469278707, Test FP rate: 0.02904, Test FN rate: 0.10722\n"
     ]
    }
   ],
   "source": [
    "# evaluate on training set\n",
    "xy = evaluate(train_dataloader, model, tau=0.2, device=device, verbose=True, return_xy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false negatives: 3019\n"
     ]
    }
   ],
   "source": [
    "tau = 0.2\n",
    "\n",
    "# get all the false negatives\n",
    "FN_urls = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for url in malicious_urls:\n",
    "        x = [char2idx[char] for char in url]\n",
    "        x = torch.tensor(x, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        outputs = model(x)[0]\n",
    "        predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "        if predicted[1].item() < tau:\n",
    "            FN_urls.append(url)\n",
    "\n",
    "print(f\"Number of false negatives: {len(FN_urls)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a backup bloom filter on all the false negative URLs predicted by the oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of hash functions: 7\n",
      "Bloom filter constructed! Size: 30190, Number of hash functions: 7\n",
      "False Positive Rate: 0.008\n",
      "False Negative Rate: 0.0\n",
      "Memory usage for bad URLs: 303.072 kilobytes, Memory usage for Backup Bloom filter: 3.856 kilobytes\n"
     ]
    }
   ],
   "source": [
    "# construct a bloom filter on the bad URLs\n",
    "backup_bf = BloomStandard(S=FN_urls, m=10*len(FN_urls))\n",
    "\n",
    "# draw some random negative samples from the good URLs\n",
    "neg_samples = random.sample(benign_urls, 5000)\n",
    "\n",
    "# evaluate the bloom filter on the negative samples\n",
    "FP_rate = sum([backup_bf.query(url) for url in neg_samples])/len(neg_samples)\n",
    "print(f\"False Positive Rate: {FP_rate}\")\n",
    "\n",
    "FN_rate = 1-sum([backup_bf.query(url) for url in FN_urls])/len(FN_urls)\n",
    "print(f\"False Negative Rate: {FN_rate}\")\n",
    "\n",
    "print(f\"Memory usage for bad URLs: {asizeof.asizeof(FN_urls)/1000} kilobytes, Memory usage for Backup Bloom filter: {asizeof.asizeof(backup_bf.B)/1000} kilobytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedBF:\n",
    "    def __init__(self, oracle, backup_bf, char2idx, tau=0.5, device='cuda'):\n",
    "        self.oracle = oracle\n",
    "        self.backup_bf = backup_bf\n",
    "        self.char2idx = char2idx\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, key):\n",
    "        oracle_score = self.oracle_predict(key)\n",
    "        if oracle_score >= self.tau:\n",
    "            return True, oracle_score\n",
    "        else:\n",
    "            return self.backup_bf.query(key), oracle_score\n",
    "\n",
    "    def oracle_predict(self, x):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = [self.char2idx[char] for char in x]\n",
    "            x = torch.tensor(x, dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "            outputs = self.oracle(x)[0]\n",
    "            predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "            return predicted[1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbf = LearnedBF(model, backup_bf, char2idx, tau=0.2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random query stream containing good and bad URLs\n",
    "benign_queries = random.sample(benign_urls, 5000)\n",
    "malicious_queries = random.sample(malicious_urls, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP rate: 0.1674, FN rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the learned bloom filter on the query stream\n",
    "num_FP = 0\n",
    "num_FN = 0\n",
    "for query in benign_queries:\n",
    "    prediction, score = lbf.predict(query)\n",
    "    if prediction:\n",
    "        num_FP += 1 \n",
    "\n",
    "for query in malicious_queries:\n",
    "    prediction, score = lbf.predict(query)\n",
    "    if not prediction:\n",
    "        num_FN += 1\n",
    "\n",
    "FP_rate = num_FP / len(benign_queries)\n",
    "FN_rate = num_FN / len(malicious_queries)\n",
    "\n",
    "print(f\"FP rate: {FP_rate}, FN rate: {FN_rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
