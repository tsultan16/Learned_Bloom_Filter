{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learned Bloom Filter - Malicious URLs Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sympy as sp\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import xxhash\n",
    "\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoUniversalHashFamily:\n",
    "    def __init__(self, m, max_key):\n",
    "        self.m = m  # Size of the hash table\n",
    "        self.p = sp.nextprime(max_key)  # generate a large prime number, greater than any key\n",
    "        self.a = random.randint(1, self.p-1)  # Choose a randomly\n",
    "        self.b = random.randint(0, self.p-1)  # Choose b randomly\n",
    "\n",
    "    def hash(self, k):\n",
    "        return ((self.a * k + self.b) % self.p) % self.m\n",
    "    \n",
    "    def __call__(self, k):\n",
    "        return self.hash(k)\n",
    "\n",
    "\n",
    "class BloomStandard:\n",
    "    def __init__(self, S, m, k=None):\n",
    "        self.m = m\n",
    "        if k is None:\n",
    "            self.k = max(1,round((m/len(S)) * math.log(2)))  # optimal number of hash functions for a given m and n \n",
    "            print(f\"Optimal number of hash functions: {self.k}\")      \n",
    "        else:\n",
    "            self.k = k\n",
    "\n",
    "        self.B = [0] * self.m\n",
    "\n",
    "        # construct bit array\n",
    "        for key in S:\n",
    "            self.insert(key)\n",
    "\n",
    "        print(f\"Bloom filter constructed! Size: {self.m}, Number of hash functions: {self.k}\")\n",
    "\n",
    "\n",
    "    # insert new integer key into the bloom filter \n",
    "    def insert(self, key):\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            self.B[hash_val] = 1\n",
    "\n",
    "    # poerform membership query for the given key\n",
    "    def query(self, key):\n",
    "        q = [0]*self.k\n",
    "        for i in range(self.k):\n",
    "            hash_val = xxhash.xxh3_64(key.encode('utf-8'), seed=i).intdigest()%self.m \n",
    "            q[i] = self.B[hash_val]\n",
    "        if 0 in q:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "            \n",
    "    def __str__(self):\n",
    "        return str(self.B)\n",
    "\n",
    "\n",
    "# define pytroch dataset class for training\n",
    "class BloomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, positive_samples, negative_samples):\n",
    "        if positive_samples is None:\n",
    "            positive_samples = []\n",
    "        self.samples = positive_samples + negative_samples\n",
    "        random.shuffle(self.samples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        # convert to tensor\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# define neural network with two hidden layers\n",
    "class Oracle(torch.nn.Module):\n",
    "    def __init__(self, hidden_dims=10, dropout_rate=0.2):\n",
    "        super(Oracle, self).__init__()\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.hidden_layer1 = torch.nn.Linear(1, hidden_dims)\n",
    "        self.hidden_layer2 = torch.nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        #self.batch_norm = torch.nn.BatchNorm1d(hidden_dims)\n",
    "        #self.leaky_relu = torch.nn.LeakyReLU(0.01)\n",
    "        self.output_layer = torch.nn.Linear(hidden_dims, 2)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        x = self.relu(x)\n",
    "        #x = self.tanh(x)\n",
    "        #x = self.batch_norm(x)\n",
    "        #x = self.leaky_relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(train_dataloader, model, optimizer, num_epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "        num_correct, num_pos_correct, num_neg_correct = 0, 0, 0\n",
    "        avg_loss = 0\n",
    "        for batch in pbar:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            # flatten the labels\n",
    "            y = y.view(-1)      \n",
    "            loss = F.cross_entropy(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute moving average loss\n",
    "            avg_loss = 0.9 * avg_loss + 0.1 * loss.item()\n",
    "\n",
    "            # compute accuracy for batch precictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            accuracy = num_correct / num_total\n",
    "\n",
    "            # compute accuracy for positive samples\n",
    "            num_pos_total += y[y == 1].shape[0]\n",
    "            num_pos_correct += (predicted[y == 1] == 1).sum().item()\n",
    "            pos_accuracy = num_pos_correct / max(1,num_pos_total)\n",
    "\n",
    "            # compute accuracy for negative samples\n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "            num_neg_correct += (predicted[y == 0] == 0).sum().item()\n",
    "            neg_accuracy = num_neg_correct / max(1,num_neg_total)  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Train Loss: {avg_loss:.5f}, Train Accuracy Overall: {accuracy: .5f}, Train Accuracy Positive: {pos_accuracy:.5f}, Train Accuracy Negative: {neg_accuracy:.5f}\")\n",
    "\n",
    "\n",
    "# evaluation on test samples\n",
    "def evaluate(test_dataloader, model, tau=0.5, device='cpu'):\n",
    "    num_total, num_pos_total, num_neg_total = 0, 0, 0\n",
    "    num_correct = 0\n",
    "    num_FP, num_FN = 0, 0\n",
    "    model.eval()\n",
    "    xy = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            predicted = (outputs[:, 1] >= tau).long()\n",
    "            \n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (predicted == y).sum().item()\n",
    "            num_pos_total += y[y == 1].shape[0]            \n",
    "            num_neg_total += y[y == 0].shape[0]\n",
    "\n",
    "            # compute number of false positives and false negatives\n",
    "            num_FP += (predicted[y == 0] == 1).sum().item()\n",
    "            num_FN += (predicted[y == 1] == 0).sum().item() \n",
    "\n",
    "            xy.extend(list(zip(x.cpu().numpy(), y.cpu().numpy(), predicted.cpu().numpy())))\n",
    "\n",
    "    print(f\"Num total: {num_total}, Num correct: {num_correct}, Num False Positives: {num_FP}, Num False Negatives: {num_FN}\")\n",
    "\n",
    "    accuracy = num_correct / num_total\n",
    "    FP_rate = num_FP / max(1,num_neg_total)\n",
    "    FN_rate = num_FN / max(1,num_pos_total)                 \n",
    "    print(f\"Test Accuracy: {accuracy}, Test FP rate: {FP_rate:.5f}, Test FN rate: {FN_rate:.5f}\")\n",
    "    return FP_rate, xy\n",
    "\n",
    "\n",
    "class LearnedBF:\n",
    "    def __init__(self, oracle, backup_bf, x_mean, x_std, tau=0.5, device='cuda'):\n",
    "        self.oracle = oracle\n",
    "        self.backup_bf = backup_bf\n",
    "        self.tau = tau\n",
    "        self.x_mean = x_mean\n",
    "        self.x_std = x_std\n",
    "        self.device = device\n",
    "\n",
    "    def query(self, key):\n",
    "        if self.oracle_predict(key):\n",
    "            return True\n",
    "        else:\n",
    "            return self.backup_bf.query(key)\n",
    "\n",
    "    def oracle_predict(self, x):\n",
    "        x = (key - self.x_mean) / self.x_std\n",
    "        x = torch.tensor(x, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        outputs = self.oracle(x)\n",
    "        predicted = torch.nn.Softmax(dim=0)(outputs)\n",
    "        return predicted[1].item() > self.tau\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file_path = 'datasets/URLs_Singh/Webpages_Classification_train_data.csv/Webpages_Classification_train_data.csv'\n",
    "\n",
    "# load only 'url' and 'label' columns\n",
    "columns_to_load = [1, 11]\n",
    "\n",
    "# Initialize empty lists to hold the data of the two columns\n",
    "urls = []\n",
    "labels = []\n",
    "\n",
    "# Open the CSV file and read line by line\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        # Append the data from the specified columns to the lists\n",
    "        urls.append(row[columns_to_load[0]])\n",
    "        labels.append(row[columns_to_load[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good URLs: 1172747, Number of bad URLs: 27253\n"
     ]
    }
   ],
   "source": [
    "num_good = sum([1 for l in labels if l == 'good'])\n",
    "num_bad = sum([1 for l in labels if l == 'bad'])\n",
    "\n",
    "print(f\"Number of good URLs: {num_good}, Number of bad URLs: {num_bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200001\n",
      "['url', 'http://members.tripod.com/russiastation/', 'http://www.ddj.com/cpp/184403822', 'http://www.naef-usa.com/', 'http://www.ff-b2b.de/']\n",
      "['label', 'good', 'good', 'good', 'bad']\n"
     ]
    }
   ],
   "source": [
    "print(len(urls))  # Print the number of elements in the columns\n",
    "\n",
    "# Now column1_data and column2_data contain the data from the specified columns\n",
    "print(urls[:5])  # Print first 5 elements of column1_data\n",
    "print(labels[:5])  # Print first 5 elements of column2_dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
